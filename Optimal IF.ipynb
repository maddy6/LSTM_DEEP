{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f50925c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   maturity_value  category  currency  amount  maturity_value_quantile_25  \\\n",
      "0            1000         3         2     500                      1200.0   \n",
      "1            2000         1         0     800                      1200.0   \n",
      "2            1500         0         2     600                      1200.0   \n",
      "3            1200         4         1     700                      1200.0   \n",
      "4            1800         2         0     900                      1200.0   \n",
      "\n",
      "   maturity_value_quantile_75  maturity_value_weighted_distance  \\\n",
      "0                      1800.0                             320.0   \n",
      "1                      1800.0                             320.0   \n",
      "2                      1800.0                             320.0   \n",
      "3                      1800.0                             320.0   \n",
      "4                      1800.0                             320.0   \n",
      "\n",
      "   maturity_value_anomaly_score  maturity_value_anomaly_score_rank  \\\n",
      "0                     -1.212678                                5.0   \n",
      "1                      1.212678                                1.0   \n",
      "2                      0.000000                                3.0   \n",
      "3                     -0.727607                                4.0   \n",
      "4                      0.727607                                2.0   \n",
      "\n",
      "   category_quantile_25  ...  currency_quantile_25  currency_quantile_75  \\\n",
      "0                   1.0  ...                   0.0                   2.0   \n",
      "1                   1.0  ...                   0.0                   2.0   \n",
      "2                   1.0  ...                   0.0                   2.0   \n",
      "3                   1.0  ...                   0.0                   2.0   \n",
      "4                   1.0  ...                   0.0                   2.0   \n",
      "\n",
      "   currency_weighted_distance  currency_anomaly_score  \\\n",
      "0                         0.8                     1.0   \n",
      "1                         0.8                    -1.0   \n",
      "2                         0.8                     1.0   \n",
      "3                         0.8                     0.0   \n",
      "4                         0.8                    -1.0   \n",
      "\n",
      "   currency_anomaly_score_rank  amount_quantile_25  amount_quantile_75  \\\n",
      "0                          1.5               600.0               800.0   \n",
      "1                          4.5               600.0               800.0   \n",
      "2                          1.5               600.0               800.0   \n",
      "3                          3.0               600.0               800.0   \n",
      "4                          4.5               600.0               800.0   \n",
      "\n",
      "   amount_weighted_distance  amount_anomaly_score  amount_anomaly_score_rank  \n",
      "0                     120.0             -1.264911                        5.0  \n",
      "1                     120.0              0.632456                        2.0  \n",
      "2                     120.0             -0.632456                        4.0  \n",
      "3                     120.0              0.000000                        3.0  \n",
      "4                     120.0              1.264911                        1.0  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.stats import percentileofscore\n",
    "\n",
    "# Sample DataFrame (replace this with your actual data)\n",
    "data = {\n",
    "    'maturity_value': [1000, 2000, 1500, 1200, 1800],\n",
    "    'category': ['S.A', 'I.T', 'D.A', 'T.H', 'J.K'],\n",
    "    'currency': ['USD', 'GBP', 'USD', 'JPY', 'GBP'],\n",
    "    'amount': [500, 800, 600, 700, 900]\n",
    "}\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "categorical_vars = ['category', 'currency']\n",
    "for var in categorical_vars:\n",
    "    le = LabelEncoder()\n",
    "    data[var] = le.fit_transform(data[var])\n",
    "    label_encoders[var] = le\n",
    "\n",
    "# Parameters for nearest neighbor features\n",
    "k_neighbors = 3  # Adjust the number of neighbors as needed\n",
    "\n",
    "# List of variables for which to calculate features\n",
    "variables_to_process = ['maturity_value', 'category', 'currency', 'amount']\n",
    "\n",
    "df = data\n",
    "\n",
    "# Loop through each variable and calculate features\n",
    "for var in variables_to_process:\n",
    "    \n",
    "    # Calculate quantile-based features\n",
    "    df[f'{var}_quantile_25'] = np.percentile(df[var], 25)\n",
    "    df[f'{var}_quantile_75'] = np.percentile(df[var], 75)\n",
    "    \n",
    "    # Calculate weighted distance features\n",
    "    df[f'{var}_weighted_distance'] = np.sum(np.sqrt((df[var] - df[var].mean())**2)) / df.shape[0]\n",
    "    \n",
    "    # Calculate anomaly score features (using example calculations)\n",
    "    df[f'{var}_anomaly_score'] = (df[var] - df[var].mean()) / df[var].std()\n",
    "    df[f'{var}_anomaly_score_rank'] = df[f'{var}_anomaly_score'].rank(ascending=False)\n",
    "    \n",
    "\n",
    "# Print the resulting DataFrame with added features\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed094ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['maturity_value', 'category', 'currency', 'amount',\n",
      "       'maturity_value_quantile_25'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming df is your dataset\n",
    "data = df.values\n",
    "variable_names = df.columns  # Get the names of the variables\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.95)  # You can adjust the explained variance threshold\n",
    "data_reduced = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Get the selected features (principal components)\n",
    "selected_features = data_reduced\n",
    "\n",
    "# Get the names of the variables corresponding to the selected principal components\n",
    "selected_variable_names = variable_names[:len(selected_features)]\n",
    "\n",
    "# Print the selected variable names\n",
    "print(selected_variable_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b04801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import silhouette_score\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X is your data and you want to find optimal contamination for Isolation Forest\n",
    "\n",
    "# Initialize a list to store silhouette scores\n",
    "silhouette_scores = []\n",
    "\n",
    "# Contamination values to test\n",
    "contamination_values = np.arange(0.01, 0.5, 0.01)\n",
    "\n",
    "def calculate_silhouette_score(contamination, X):\n",
    "    # Create and fit the Isolation Forest model\n",
    "    model = IsolationForest(contamination=contamination, n_estimators=100, max_samples=0.5)\n",
    "    model.fit(X)\n",
    "    \n",
    "    # Predict anomalies\n",
    "    predictions = model.predict(X)\n",
    "    \n",
    "    # Calculate the silhouette score\n",
    "    return silhouette_score(X, predictions)\n",
    "\n",
    "# Use Parallel to calculate silhouette scores in parallel\n",
    "num_jobs = -1  # Number of CPU cores to use, -1 uses all available cores\n",
    "silhouette_scores = Parallel(n_jobs=num_jobs)(delayed(calculate_silhouette_score)(contamination, X) for contamination in contamination_values)\n",
    "\n",
    "# Find the optimal contamination value that maximizes the silhouette score\n",
    "optimal_contamination = contamination_values[np.argmax(silhouette_scores)]\n",
    "\n",
    "print(\"Optimal Contamination:\", optimal_contamination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d73bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d625d168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.7720\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.7671\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.7623\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.7574\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7526\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "   maturity_value  category  currency  amount  maturity_value_quantile_25  \\\n",
      "0            1000         3         2     500                      1200.0   \n",
      "1            2000         1         0     800                      1200.0   \n",
      "2            1500         0         2     600                      1200.0   \n",
      "3            1200         4         1     700                      1200.0   \n",
      "4            1800         2         0     900                      1200.0   \n",
      "\n",
      "   maturity_value_quantile_75  maturity_value_weighted_distance  \\\n",
      "0                      1800.0                             320.0   \n",
      "1                      1800.0                             320.0   \n",
      "2                      1800.0                             320.0   \n",
      "3                      1800.0                             320.0   \n",
      "4                      1800.0                             320.0   \n",
      "\n",
      "   maturity_value_anomaly_score  maturity_value_anomaly_score_rank  \\\n",
      "0                     -1.212678                                5.0   \n",
      "1                      1.212678                                1.0   \n",
      "2                      0.000000                                3.0   \n",
      "3                     -0.727607                                4.0   \n",
      "4                      0.727607                                2.0   \n",
      "\n",
      "   category_quantile_25  ...  currency_quantile_75  \\\n",
      "0                   1.0  ...                   2.0   \n",
      "1                   1.0  ...                   2.0   \n",
      "2                   1.0  ...                   2.0   \n",
      "3                   1.0  ...                   2.0   \n",
      "4                   1.0  ...                   2.0   \n",
      "\n",
      "   currency_weighted_distance  currency_anomaly_score  \\\n",
      "0                         0.8                     1.0   \n",
      "1                         0.8                    -1.0   \n",
      "2                         0.8                     1.0   \n",
      "3                         0.8                     0.0   \n",
      "4                         0.8                    -1.0   \n",
      "\n",
      "   currency_anomaly_score_rank  amount_quantile_25  amount_quantile_75  \\\n",
      "0                          1.5               600.0               800.0   \n",
      "1                          4.5               600.0               800.0   \n",
      "2                          1.5               600.0               800.0   \n",
      "3                          3.0               600.0               800.0   \n",
      "4                          4.5               600.0               800.0   \n",
      "\n",
      "   amount_weighted_distance  amount_anomaly_score  amount_anomaly_score_rank  \\\n",
      "0                     120.0             -1.264911                        5.0   \n",
      "1                     120.0              0.632456                        2.0   \n",
      "2                     120.0             -0.632456                        4.0   \n",
      "3                     120.0              0.000000                        3.0   \n",
      "4                     120.0              1.264911                        1.0   \n",
      "\n",
      "   anomaly  \n",
      "0     True  \n",
      "1    False  \n",
      "2    False  \n",
      "3    False  \n",
      "4    False  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming df is your dataset\n",
    "data = df.values\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Define the architecture of the Autoencoder\n",
    "input_dim = data_scaled.shape[1]\n",
    "encoding_dim = 32  # You can adjust this based on your needs\n",
    "\n",
    "input_layer = tf.keras.layers.Input(shape=(input_dim,))\n",
    "encoder_layer = tf.keras.layers.Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoder_layer = tf.keras.layers.Dense(input_dim, activation='sigmoid')(encoder_layer)\n",
    "\n",
    "autoencoder = tf.keras.models.Model(inputs=input_layer, outputs=decoder_layer)\n",
    "\n",
    "# Compile the Autoencoder model\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the Autoencoder using parallel processing\n",
    "autoencoder.fit(data_scaled, data_scaled, epochs=5, batch_size=64, shuffle=True, use_multiprocessing=True, workers=-1)\n",
    "\n",
    "# Use the trained Autoencoder for anomaly detection\n",
    "encoded_data = autoencoder.predict(data_scaled)\n",
    "mse = np.mean(np.power(data_scaled - encoded_data, 2), axis=1)\n",
    "\n",
    "# Calculate the optimal threshold using quantiles\n",
    "optimal_quantile = 95  # You can adjust this based on your needs\n",
    "threshold = np.percentile(mse, optimal_quantile)\n",
    "\n",
    "# Identify anomalies\n",
    "anomalies = mse > threshold\n",
    "\n",
    "# Add an \"anomaly\" column to the original DataFrame\n",
    "df['anomaly'] = anomalies\n",
    "\n",
    "# Print the DataFrame with the added \"anomaly\" column\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "721ae34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    4\n",
       "True     1\n",
       "Name: anomaly, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['anomaly'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1376796e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
