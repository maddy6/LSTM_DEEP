{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8449e93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     transaction_time      amount  user_id  fraud_tag  \\\n",
      "0 2022-01-01 00:00:00  380.794718        7          0   \n",
      "1 2022-01-01 01:00:00  951.207163        2          1   \n",
      "2 2022-01-01 02:00:00  734.674002        7          1   \n",
      "3 2022-01-01 03:00:00  602.671899        4          0   \n",
      "4 2022-01-01 04:00:00  164.458454        0          1   \n",
      "\n",
      "   time_since_last_expiration  count_apple_ids_24hr  count_apple_ids_30days  \\\n",
      "0                          17                     1                      10   \n",
      "1                          10                     3                       1   \n",
      "2                          11                     0                      15   \n",
      "3                          22                     1                       8   \n",
      "4                           9                     0                      14   \n",
      "\n",
      "   days_since_card_active  avs_address_mismatch  days_since_address_change  \\\n",
      "0                     225                     0                        129   \n",
      "1                     313                     1                         87   \n",
      "2                     103                     0                        330   \n",
      "3                     347                     1                        153   \n",
      "4                     301                     1                        182   \n",
      "\n",
      "   days_since_phone_change  transaction_velocity  user_behavior_consistency  \\\n",
      "0                      354                   NaN                        NaN   \n",
      "1                      333                   NaN                        NaN   \n",
      "2                      292             -0.706753                  -0.321590   \n",
      "3                      345              0.709592                  -0.154987   \n",
      "4                      152              0.001419                  -0.434366   \n",
      "\n",
      "   sequential_pattern  transaction_density_social_network  \\\n",
      "0                 NaN                            1.481011   \n",
      "1                 NaN                           -0.136991   \n",
      "2            1.140672                            1.481011   \n",
      "3            1.576124                            0.879270   \n",
      "4            0.032065                            0.139909   \n",
      "\n",
      "   temporal_anomaly_score  \n",
      "0               -0.556621  \n",
      "1                1.556155  \n",
      "2                0.750674  \n",
      "3                0.291315  \n",
      "4               -1.098364  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a synthetic dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "data = {\n",
    "    'transaction_time': pd.date_range(start='2022-01-01', periods=n_samples, freq='H'),\n",
    "    'amount': np.random.uniform(10, 1000, n_samples),\n",
    "    'user_id': np.random.choice(10, 1000, n_samples),\n",
    "    'fraud_tag': np.random.choice([0, 1], size=n_samples),\n",
    "    'time_since_last_expiration': np.random.randint(1, 30, n_samples),\n",
    "    'count_apple_ids_24hr': np.random.randint(0, 5, n_samples),\n",
    "    'count_apple_ids_30days': np.random.randint(0, 20, n_samples),\n",
    "    'days_since_card_active': np.random.randint(1, 365, n_samples),\n",
    "    'avs_address_mismatch': np.random.choice([0, 1], size=n_samples),\n",
    "    'days_since_address_change': np.random.randint(1, 365, n_samples),\n",
    "    'days_since_phone_change': np.random.randint(1, 365, n_samples),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Example feature engineering for a few features\n",
    "\n",
    "# 1. Transaction Velocity\n",
    "df['transaction_velocity'] = df.groupby('fraud_tag')['transaction_time'].diff().dt.total_seconds()\n",
    "\n",
    "# 2. User Behavior Consistency Index\n",
    "df['user_behavior_consistency'] = df.groupby('fraud_tag')['amount'].pct_change()\n",
    "\n",
    "# 3. Sequential Pattern Indicator\n",
    "df['sequential_pattern'] = df['amount'].rolling(window=3).mean()\n",
    "\n",
    "# 4. Social Network Features\n",
    "df['transaction_density_social_network'] = df.groupby('user_id')['amount'].transform('mean')\n",
    "\n",
    "# 5. Temporal Anomaly Score\n",
    "df['temporal_anomaly_score'] = df.groupby('user_id')['amount'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "# Standardize numeric features\n",
    "numeric_cols = ['transaction_velocity', 'user_behavior_consistency', 'sequential_pattern',\n",
    "                'transaction_density_social_network', 'temporal_anomaly_score']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91321413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bb715a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate random data for the sample dataset\n",
    "data = {\n",
    "    'amount': np.random.uniform(10, 1000, 2000),\n",
    "    'transaction_time': [datetime.now() - timedelta(days=np.random.randint(1, 365)) for _ in range(2000)],\n",
    "    'fraud_tag': np.random.choice([0, 1], size=2000, p=[0.95, 0.05]),\n",
    "    'time_since_last_expiration_date': np.random.randint(1, 365, 2000),\n",
    "    'count_apple_ids_24hr': np.random.randint(0, 10, 2000),\n",
    "    'count_apple_ids_30days': np.random.randint(0, 20, 2000),\n",
    "    'days_since_card_active': np.random.randint(1, 365, 2000),\n",
    "    'avs_address_mismatch': np.random.choice([0, 1], size=2000, p=[0.8, 0.2]),\n",
    "    'days_since_address_change': np.random.randint(1, 365, 2000),\n",
    "    'days_since_phone_change': np.random.randint(1, 365, 2000),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fea3c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour_of_day'] = df['transaction_time'].dt.hour\n",
    "df['day_of_week'] = df['transaction_time'].dt.dayofweek\n",
    "df['amount_category'] = pd.cut(df['amount'], bins=[0, 100, 500, 1000], labels=['low', 'medium', 'high'])\n",
    "df['transaction_count_24hr'] = df['transaction_time'].apply(lambda x: df[(df['transaction_time'] > x - timedelta(hours=24)) & (df['transaction_time'] <= x)].shape[0])\n",
    "df['transaction_count_7days'] = df['transaction_time'].apply(lambda x: df[(df['transaction_time'] > x - timedelta(days=7)) & (df['transaction_time'] <= x)].shape[0])\n",
    "df['card_age_category'] = pd.cut(df['days_since_card_active'], bins=[0, 30, 180, 365], labels=['new', 'medium', 'old'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fcb2f9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1923\n",
       "1      77\n",
       "Name: fraud_tag, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['fraud_tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8dc628d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>transaction_time</th>\n",
       "      <th>fraud_tag</th>\n",
       "      <th>time_since_last_expiration_date</th>\n",
       "      <th>count_apple_ids_24hr</th>\n",
       "      <th>count_apple_ids_30days</th>\n",
       "      <th>days_since_card_active</th>\n",
       "      <th>avs_address_mismatch</th>\n",
       "      <th>days_since_address_change</th>\n",
       "      <th>days_since_phone_change</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>amount_category</th>\n",
       "      <th>transaction_count_24hr</th>\n",
       "      <th>transaction_count_7days</th>\n",
       "      <th>card_age_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>380.794718</td>\n",
       "      <td>2023-06-10 08:59:29.208167</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>232</td>\n",
       "      <td>225</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>medium</td>\n",
       "      <td>7</td>\n",
       "      <td>35</td>\n",
       "      <td>old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>951.207163</td>\n",
       "      <td>2023-08-06 08:59:29.208167</td>\n",
       "      <td>0</td>\n",
       "      <td>196</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>302</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>310</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>high</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "      <td>old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>734.674002</td>\n",
       "      <td>2023-06-10 08:59:29.208167</td>\n",
       "      <td>0</td>\n",
       "      <td>327</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>61</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>high</td>\n",
       "      <td>7</td>\n",
       "      <td>35</td>\n",
       "      <td>old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>602.671899</td>\n",
       "      <td>2023-04-21 08:59:29.208167</td>\n",
       "      <td>0</td>\n",
       "      <td>318</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>high</td>\n",
       "      <td>7</td>\n",
       "      <td>41</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>164.458454</td>\n",
       "      <td>2023-09-18 08:59:29.208167</td>\n",
       "      <td>0</td>\n",
       "      <td>297</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>220</td>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "      <td>229</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>medium</td>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>old</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       amount           transaction_time  fraud_tag  \\\n",
       "0  380.794718 2023-06-10 08:59:29.208167          0   \n",
       "1  951.207163 2023-08-06 08:59:29.208167          0   \n",
       "2  734.674002 2023-06-10 08:59:29.208167          0   \n",
       "3  602.671899 2023-04-21 08:59:29.208167          0   \n",
       "4  164.458454 2023-09-18 08:59:29.208167          0   \n",
       "\n",
       "   time_since_last_expiration_date  count_apple_ids_24hr  \\\n",
       "0                               17                     9   \n",
       "1                              196                     1   \n",
       "2                              327                     9   \n",
       "3                              318                     5   \n",
       "4                              297                     5   \n",
       "\n",
       "   count_apple_ids_30days  days_since_card_active  avs_address_mismatch  \\\n",
       "0                      15                     187                     0   \n",
       "1                      13                     302                     0   \n",
       "2                       0                     200                     1   \n",
       "3                      15                      39                     0   \n",
       "4                      17                     220                     1   \n",
       "\n",
       "   days_since_address_change  days_since_phone_change  hour_of_day  \\\n",
       "0                        232                      225            8   \n",
       "1                         89                      310            8   \n",
       "2                         77                       61            8   \n",
       "3                        136                       16            8   \n",
       "4                        110                      229            8   \n",
       "\n",
       "   day_of_week amount_category  transaction_count_24hr  \\\n",
       "0            5          medium                       7   \n",
       "1            6            high                       8   \n",
       "2            5            high                       7   \n",
       "3            4            high                       7   \n",
       "4            0          medium                       4   \n",
       "\n",
       "   transaction_count_7days card_age_category  \n",
       "0                       35               old  \n",
       "1                       44               old  \n",
       "2                       35               old  \n",
       "3                       41            medium  \n",
       "4                       42               old  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28da6da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       384\n",
      "           1       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.96       400\n",
      "   macro avg       0.48      0.50      0.49       400\n",
      "weighted avg       0.92      0.96      0.94       400\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[384   0]\n",
      " [ 16   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pixel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\pixel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\pixel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Define features and target variable\n",
    "features = df.drop(['fraud_tag', 'transaction_time'], axis=1)\n",
    "target = df['fraud_tag']\n",
    "\n",
    "# Convert categorical variables to dummy/indicator variables\n",
    "features = pd.get_dummies(features)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc1ab459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Feature  Importance\n",
      "0                            amount    0.131527\n",
      "6         days_since_address_change    0.130677\n",
      "7           days_since_phone_change    0.114428\n",
      "4            days_since_card_active    0.112385\n",
      "1   time_since_last_expiration_date    0.112023\n",
      "11          transaction_count_7days    0.085893\n",
      "3            count_apple_ids_30days    0.071396\n",
      "10           transaction_count_24hr    0.065104\n",
      "2              count_apple_ids_24hr    0.060014\n",
      "9                       day_of_week    0.051540\n",
      "16         card_age_category_medium    0.011766\n",
      "13           amount_category_medium    0.010860\n",
      "14             amount_category_high    0.010739\n",
      "17            card_age_category_old    0.009958\n",
      "5              avs_address_mismatch    0.009740\n",
      "15            card_age_category_new    0.006314\n",
      "12              amount_category_low    0.005634\n",
      "8                       hour_of_day    0.000000\n"
     ]
    }
   ],
   "source": [
    "# Get variable importance\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display the feature importance\n",
    "feature_importance_df = pd.DataFrame({'Feature': features.columns, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the variable importance\n",
    "print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b845238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c8cef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a997c1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "RFE\n",
      "========================================\n",
      "Variable Importance or Score with RFE:\n",
      "                           Feature  Importance\n",
      "1                 transaction_time    0.367466\n",
      "0                           amount    0.348696\n",
      "2  time_since_last_expiration_date    0.283838\n",
      "Model Performance on Reduced Features:\n",
      "Accuracy: 0.94\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97       377\n",
      "           1       0.00      0.00      0.00        23\n",
      "\n",
      "    accuracy                           0.94       400\n",
      "   macro avg       0.47      0.50      0.48       400\n",
      "weighted avg       0.89      0.94      0.91       400\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[376   1]\n",
      " [ 23   0]]\n",
      "\n",
      "========================================\n",
      "Mutual Information\n",
      "========================================\n",
      "Mutual Information does not provide variable importance information.\n",
      "Model Performance on Reduced Features:\n",
      "Accuracy: 0.94\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97       377\n",
      "           1       0.00      0.00      0.00        23\n",
      "\n",
      "    accuracy                           0.94       400\n",
      "   macro avg       0.47      0.50      0.48       400\n",
      "weighted avg       0.89      0.94      0.91       400\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[376   1]\n",
      " [ 23   0]]\n",
      "\n",
      "========================================\n",
      "LASSO\n",
      "========================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Lasso' object has no attribute 'fit_transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Fit the feature reduction method\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m X_train_reduced \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m(X_train_scaled, y_train)\n\u001b[0;32m     67\u001b[0m X_test_reduced \u001b[38;5;241m=\u001b[39m method\u001b[38;5;241m.\u001b[39mtransform(X_test_scaled)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Display the variable importance or feature selection scores if available\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Lasso' object has no attribute 'fit_transform'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE, SelectKBest, mutual_info_classif\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_selection import RFECV\n",
    "#from boruta import BorutaPy\n",
    "#import umap\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Generating synthetic data for demonstration\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'amount': np.random.uniform(10, 1000, 2000),\n",
    "    'transaction_time': np.random.uniform(0, 1, 2000),\n",
    "    'fraud_tag': np.random.choice([0, 1], size=2000, p=[0.95, 0.05]),\n",
    "    'time_since_last_expiration_date': np.random.randint(1, 365, 2000),\n",
    "    # Add other features as needed\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "def evaluate_model(X_train_reduced, X_test_reduced, model):\n",
    "    model.fit(X_train_reduced, y_train)\n",
    "    y_pred = model.predict(X_test_reduced)\n",
    "\n",
    "    print(\"Model Performance on Reduced Features:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    \n",
    "# Assuming 'fraud_tag' is the target variable\n",
    "features = df.drop(['fraud_tag'], axis=1)\n",
    "target = df['fraud_tag']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define a list of feature reduction methods\n",
    "feature_reduction_methods = [\n",
    "    (\"RFE\", RFE(RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=5)),\n",
    "    (\"Mutual Information\", SelectKBest(score_func=mutual_info_classif, k=3)),\n",
    "    (\"LASSO\", Lasso(alpha=0.1)),\n",
    "    (\"Kernel PCA\", KernelPCA(n_components=2, kernel='rbf')),\n",
    "    (\"t-SNE\", TSNE(n_components=2, random_state=42)),\n",
    "    (\"RFECV\", RFECV(RandomForestClassifier(n_estimators=100, random_state=42), step=1, cv=5, scoring='accuracy')),\n",
    "    #(\"Boruta\", BorutaPy(RandomForestClassifier(n_estimators='auto', random_state=42), n_estimators='auto', verbose=2)),\n",
    "    #(\"UMAP\", umap.UMAP(n_components=2, random_state=42))\n",
    "]\n",
    "\n",
    "# Evaluate each feature reduction method\n",
    "for method_name, method in feature_reduction_methods:\n",
    "    print(f\"\\n{'='*40}\\n{method_name}\\n{'='*40}\")\n",
    "\n",
    "    # Fit the feature reduction method\n",
    "    X_train_reduced = method.fit_transform(X_train_scaled, y_train)\n",
    "    X_test_reduced = method.transform(X_test_scaled)\n",
    "\n",
    "    # Display the variable importance or feature selection scores if available\n",
    "    if hasattr(method, 'support_') and method_name != \"t-SNE\":\n",
    "        # For methods with support (excluding t-SNE)\n",
    "        if hasattr(method, 'estimator_'):\n",
    "            # For methods with an estimator (e.g., RFE)\n",
    "            try:\n",
    "                importances = method.estimator_.feature_importances_\n",
    "            except AttributeError:\n",
    "                importances = None\n",
    "        else:\n",
    "            # For methods without an estimator (e.g., SelectKBest)\n",
    "            importances = method.scores_\n",
    "        if importances is not None:\n",
    "            feature_names = X_train.columns[method.support_]\n",
    "            variable_importance = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "            variable_importance = variable_importance.sort_values(by='Importance', ascending=False)\n",
    "            print(f\"Variable Importance or Score with {method_name}:\\n{variable_importance}\")\n",
    "        else:\n",
    "            print(f\"{method_name} does not provide variable importance information.\")\n",
    "    else:\n",
    "        print(f\"{method_name} does not provide variable importance information.\")\n",
    "\n",
    "    # Evaluate the model with reduced features\n",
    "    evaluate_model(X_train_reduced, X_test_reduced, RandomForestClassifier(n_estimators=100, random_state=42))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02adf5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features based on Mutual Information: Index(['count_apple_ids_24hr', 'count_apple_ids_30days',\n",
      "       'days_since_card_active'],\n",
      "      dtype='object')\n",
      "Selected Features based on Mutual Information: Index(['count_apple_ids_24hr', 'count_apple_ids_30days',\n",
      "       'days_since_card_active'],\n",
      "      dtype='object')\n",
      "Values of the Selected Features: [1.57110071 0.24603328 1.14458295]\n",
      "\n",
      "Model Performance with Mutual Information-based Feature Selection:\n",
      "Accuracy: 0.9425\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97       379\n",
      "           1       0.00      0.00      0.00        21\n",
      "\n",
      "    accuracy                           0.94       400\n",
      "   macro avg       0.47      0.50      0.49       400\n",
      "weighted avg       0.90      0.94      0.92       400\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[377   2]\n",
      " [ 21   0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generating synthetic data for demonstration\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'amount': np.random.uniform(10, 1000, 2000),\n",
    "    'transaction_time': np.random.uniform(0, 1, 2000),\n",
    "    'fraud_tag': np.random.choice([0, 1], size=2000, p=[0.95, 0.05]),\n",
    "    'time_since_last_expiration_date': np.random.randint(1, 365, 2000),\n",
    "    # Add other features as needed\n",
    "}\n",
    "\n",
    "data = {\n",
    "    'amount': np.random.uniform(10, 1000, 2000),\n",
    "   # 'transaction_time': [datetime.now() - timedelta(days=np.random.randint(1, 365)) for _ in range(2000)],\n",
    "    'fraud_tag': np.random.choice([0, 1], size=2000, p=[0.95, 0.05]),\n",
    "    'time_since_last_expiration_date': np.random.randint(1, 365, 2000),\n",
    "    'count_apple_ids_24hr': np.random.randint(0, 10, 2000),\n",
    "    'count_apple_ids_30days': np.random.randint(0, 20, 2000),\n",
    "    'days_since_card_active': np.random.randint(1, 365, 2000),\n",
    "    'avs_address_mismatch': np.random.choice([0, 1], size=2000, p=[0.8, 0.2]),\n",
    "    'days_since_address_change': np.random.randint(1, 365, 2000),\n",
    "    'days_since_phone_change': np.random.randint(1, 365, 2000),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Assuming 'fraud_tag' is the target variable\n",
    "features = df.drop(['fraud_tag'], axis=1)\n",
    "target = df['fraud_tag']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select top k features based on mutual information\n",
    "k_best = 3  # Choose the desired number of features\n",
    "selector_mi = SelectKBest(score_func=mutual_info_classif, k=k_best)\n",
    "X_train_mi = selector_mi.fit_transform(X_train_scaled, y_train)\n",
    "X_test_mi = selector_mi.transform(X_test_scaled)\n",
    "\n",
    "# Display the selected features\n",
    "selected_features_mi = X_train.columns[selector_mi.get_support()]\n",
    "print(f\"Selected Features based on Mutual Information: {selected_features_mi}\")\n",
    "\n",
    "# Display the selected features and their values\n",
    "selected_features_mi = X_train.columns[selector_mi.get_support()]\n",
    "selected_values_mi = X_train_mi[0]  # Assuming you want the values of the first instance, adjust as needed\n",
    "print(f\"Selected Features based on Mutual Information: {selected_features_mi}\")\n",
    "print(f\"Values of the Selected Features: {selected_values_mi}\")\n",
    "\n",
    "\n",
    "# Evaluate a model with the selected features\n",
    "model_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_rf.fit(X_train_mi, y_train)\n",
    "y_pred_mi = model_rf.predict(X_test_mi)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"\\nModel Performance with Mutual Information-based Feature Selection:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_mi))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_mi))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_mi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78491697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7738e824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696c19a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf779d41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe74ede5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becb9320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b288d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee49552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a99697c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d6ee90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7396901b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b8a4b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9eacd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170a960b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74af61e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c190011a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c519c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83a9566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65683c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e8edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a synthetic dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "data = {\n",
    "    'user_id': np.random.choice(range(1, 101), size=n_samples),\n",
    "    'transaction_type': np.random.choice(['purchase', 'withdrawal', 'transfer'], size=n_samples),\n",
    "    'amount': np.random.uniform(10, 1000, n_samples),\n",
    "    'fraud_tag': np.random.choice([0, 1], size=n_samples),\n",
    "    'transaction_time': pd.date_range(start='2022-01-01', periods=n_samples, freq='H'),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sort the DataFrame by user_id and transaction_time\n",
    "df = df.sort_values(by=['user_id', 'transaction_type'])\n",
    "\n",
    "# User Behavior Consistency Index\n",
    "df['user_behavior_consistency'] = df.groupby('user_id')['amount'].pct_change()\n",
    "\n",
    "# Transaction Diversity\n",
    "transaction_diversity = df.groupby('user_id')['transaction_type'].nunique().reset_index()\n",
    "transaction_diversity.columns = ['user_id', 'transaction_diversity']\n",
    "\n",
    "# Merge the Transaction Diversity feature back to the main DataFrame\n",
    "df = pd.merge(df, transaction_diversity, on='user_id', how='left')\n",
    "\n",
    "# Fill NaN values with 0 (for users with only one type of transaction)\n",
    "df['transaction_diversity'].fillna(0, inplace=True)\n",
    "\n",
    "# Display the DataFrame with the new features\n",
    "print(df[['user_id', 'fraud_tag','transaction_type', 'amount', 'user_behavior_consistency', 'transaction_diversity']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db770f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deb5251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7df927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88690c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
