import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import LabelEncoder, StandardScaler

def preprocess_data(data, important_features, other_features, 
                    important_categorical_features, important_numerical_features, important_boolean_features,
                    other_categorical_features, other_numerical_features, other_boolean_features,
                    important_weightage=0.7, other_weightage=0.3):
    data.fillna(0, inplace=True)  # Replace missing values with 0 or customize
    
    important_data = data[important_features]
    
    # Handle categorical features in important_data
    for feature in important_categorical_features:
        important_data[feature] = important_data[feature].astype(str)  # Convert to object type
        label_encoder = LabelEncoder()
        important_data[feature] = label_encoder.fit_transform(important_data[feature])
    
    # Handle boolean features in important_data
    for feature in important_boolean_features:
        important_data[feature] = important_data[feature].astype(int)
    
    # Handle numeric features in important_data
    for feature in important_numerical_features:
        important_data[feature].fillna(0, inplace=True)  # Replace with custom value if needed
    
    other_data = data[other_features]
    
    # Handle categorical features in other_data
    for feature in other_categorical_features:
        other_data[feature] = other_data[feature].astype(str)  # Convert to object type
        label_encoder = LabelEncoder()
        other_data[feature] = label_encoder.fit_transform(other_data[feature])
    
    # Handle boolean features in other_data
    for feature in other_boolean_features:
        other_data[feature] = other_data[feature].astype(int)
    
    # Handle numeric features in other_data
    for feature in other_numerical_features:
        other_data[feature].fillna(0, inplace=True)  # Replace with custom value if needed
    
    combined_features = pd.concat([important_data, other_data], axis=1)
    
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(combined_features)
    
    weighted_data = (scaled_data[:, :len(important_features)] * important_weightage) + \
                    (scaled_data[:, len(important_features):] * other_weightage)
    
    return weighted_data

def detect_anomalies_iforest(data, contamination=0.05, random_seed=42):
    model = IsolationForest(contamination=contamination, random_state=random_seed)
    model.fit(data)
    anomalies = model.predict(data) == -1
    return anomalies

# Example usage
data = pd.read_csv('your_data.csv')  # Load your data

important_features = ["important_feature_1", "important_feature_2"]
other_features = ["other_feature_1", "other_feature_2"]
important_categorical_features = ["cat_feature_1", "cat_feature_2"]
important_numerical_features = ["num_feature_1", "num_feature_2"]
important_boolean_features = ["bool_feature_1", "bool_feature_2"]
other_categorical_features = ["other_cat_feature_1", "other_cat_feature_2"]
other_numerical_features = ["other_num_feature_1", "other_num_feature_2"]
other_boolean_features = ["other_bool_feature_1", "other_bool_feature_2"]

preprocessed_data = preprocess_data(data, 
                                   important_features, other_features, 
                                   important_categorical_features, important_numerical_features, important_boolean_features,
                                   other_categorical_features, other_numerical_features, other_boolean_features,
                                   important_weightage=0.7, other_weightage=0.3)

anomalies = detect_anomalies_iforest(preprocessed_data, contamination=0.05, random_seed=42)

# Print the indices of anomaly rows
anomaly_indices = np.where(anomalies)[0]
print("Anomaly indices:", anomaly_indices)




important_data = data[important_features]

# Handle categorical features in important_data
encoded_categorical_data = pd.DataFrame()  # Create an empty DataFrame to store encoded features
for feature in important_categorical_features:
    if feature in important_features:  # Ensure the feature is present in important_features
        encoded_feature = important_data[feature].astype(str)
        label_encoder = LabelEncoder()
        encoded_feature = label_encoder.fit_transform(encoded_feature)
        encoded_categorical_data[feature] = encoded_feature

important_data.drop(columns=important_categorical_features, inplace=True)  # Drop the original categorical columns
important_data = pd.concat([important_data, encoded_categorical_data], axis=1)  # Concatenate encoded categorical features





An autoencoder is a type of neural network architecture commonly used for various tasks, including anomaly detection. It is particularly effective at learning a compact representation of input data and can be used to identify anomalies or outliers in a dataset. Here's an explanation of autoencoders in the context of anomaly detection:

Basic Autoencoder Structure:
An autoencoder consists of two main parts: an encoder and a decoder. These parts are composed of neural network layers. The encoder takes the input data and compresses it into a lower-dimensional representation called the "latent space." The decoder then takes this lower-dimensional representation and attempts to reconstruct the original input data. The objective is to minimize the difference between the input data and the reconstructed data.

Training Phase:
During the training phase of an autoencoder, you provide it with a dataset containing normal, non-anomalous data. The autoencoder learns to encode and decode this normal data accurately, with the objective of minimizing a loss function that quantifies the reconstruction error (e.g., mean squared error or binary cross-entropy). The encoder learns to capture the most essential features of the data while reducing its dimensionality.

Anomalies Detection:
After training, the autoencoder can be used for anomaly detection. Here's how it works:

Encoding New Data: When you input new data into the trained autoencoder, the encoder part compresses it into the latent space representation.

Reconstruction: The decoder then tries to reconstruct the input data from this latent representation.

Comparing Reconstruction to Input: If the reconstruction closely matches the input data, it is considered normal. However, if the reconstruction significantly deviates from the input, it suggests an anomaly or outlier.

Thresholding: You can set a threshold for the reconstruction error. Data with reconstruction errors above this threshold are considered anomalies.

Advantages:

Autoencoders can capture complex, nonlinear patterns in data.
They are unsupervised, meaning they don't require labeled anomaly data for training.
They can learn a compact representation of the data, making them effective at detecting anomalies even in high-dimensional data.
Hyperparameter Tuning:
The choice of hyperparameters such as the architecture of the autoencoder (e.g., the number of layers and neurons), the size of the latent space, and the reconstruction error threshold can affect the performance of the anomaly detection.

Limitations:

Autoencoders may not perform well if the anomalies are very different from the normal data in terms of their distribution.
Selecting an appropriate threshold can be challenging and may require domain knowledge.
Autoencoders are a powerful tool for anomaly detection, especially in cases where anomalies are defined as data points that deviate significantly from the normal patterns captured during training. They have found applications in various fields, including cybersecurity, fraud detection, and quality control.
