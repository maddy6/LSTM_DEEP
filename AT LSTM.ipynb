{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"SampleDataset\").getOrCreate()\n",
    "\n",
    "# Define the schema of the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"last_email_change_days\", IntegerType(), nullable=False),\n",
    "    StructField(\"last_email_phone_number_change\", IntegerType(), nullable=False),\n",
    "    StructField(\"transaction_amount\", DoubleType(), nullable=False),\n",
    "    StructField(\"transaction_time\", TimestampType(), nullable=False),\n",
    "    StructField(\"fraud_tag\", IntegerType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Create the sample data\n",
    "data = []\n",
    "for _ in range(2000):\n",
    "    last_email_change_days = random.randint(1, 30)\n",
    "    last_email_phone_number_change = random.randint(1, 10)\n",
    "    transaction_amount = round(random.uniform(10, 500), 2)\n",
    "    transaction_time = datetime.now() - timedelta(days=random.randint(1, 30))\n",
    "    fraud_tag = random.choice([0, 1])  # 0 for non-fraud, 1 for fraud\n",
    "    data.append((last_email_change_days, last_email_phone_number_change, transaction_amount, transaction_time, fraud_tag))\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bdf41a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4915b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LSTMClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8483f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"AccountFraudDetection\").getOrCreate()\n",
    "\n",
    "data = spark.read.csv(\"path_to_dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Perform any necessary data preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4eecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"fraud_tag\", outputCol=\"label\")\n",
    "indexedData = indexer.fit(data).transform(data)\n",
    "\n",
    "featureCols = [\"last_email_change_days\", \"last_email_phone_change\", \"transaction_amount\"]\n",
    "assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\n",
    "assembledData = assembler.transform(indexedData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd19bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, testData = assembledData.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c1a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTMClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    maxIter=10,\n",
    "    stepSize=0.1,\n",
    "    blockSize=128,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[lstm])\n",
    "\n",
    "model = pipeline.fit(trainData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549ff0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(testData)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(\"Area Under ROC Curve (AUC):\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff1450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "newData = spark.read.csv(\"path_to_new_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "newPredictions = model.transform(newData)\n",
    "\n",
    "predictedLabels = newPredictions.select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n",
    "predictedProbabilities = newPredictions.select(\"probability\").rdd.flatMap(lambda x: x).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930a8f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok . Can you create or give me codes for some algorithm in pyaprk which can capture sequential fraud patterns from the dataset\n",
    "\n",
    "ChatGPT\n",
    "Certainly! To capture sequential fraud patterns from a dataset in PySpark, you can utilize algorithms such as Sequence Mining or Sequential Pattern Mining. These algorithms are commonly used for discovering frequent patterns and sequential dependencies in sequential data.\n",
    "\n",
    "One popular algorithm for sequential pattern mining is the PrefixSpan algorithm, which can be implemented in PySpark using the MLlib library. The PrefixSpan algorithm mines frequent sequential patterns efficiently using a depth-first search approach. Here's an example of how you can use the PrefixSpan algorithm in PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9e2482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.fpm import PrefixSpan\n",
    "\n",
    "# Load and preprocess the dataset (assuming it has been preprocessed and transformed into sequences)\n",
    "# sequences: DataFrame with a column named \"sequence\" that contains the sequences of events\n",
    "\n",
    "# Create an instance of the PrefixSpan algorithm\n",
    "prefixSpan = PrefixSpan(\n",
    "    minSupport=0.1,  # Minimum support threshold for frequent patterns\n",
    "    maxPatternLength=10  # Maximum length of sequential patterns to be discovered\n",
    ")\n",
    "\n",
    "# Fit the PrefixSpan model on the dataset\n",
    "prefixSpanModel = prefixSpan.fit(sequences)\n",
    "\n",
    "# Get the frequent sequential patterns\n",
    "frequentPatterns = prefixSpanModel.freqSequences\n",
    "\n",
    "# Display the frequent patterns\n",
    "frequentPatterns.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca494cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290a7ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce93a52a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e11635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, TimestampType\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LSTMClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.fpm import PrefixSpan\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"AccountFraudDetection\").getOrCreate()\n",
    "\n",
    "# Define the schema of the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"last_email_change_days\", IntegerType(), nullable=False),\n",
    "    StructField(\"last_email_phone_number_change\", IntegerType(), nullable=False),\n",
    "    StructField(\"transaction_amount\", DoubleType(), nullable=False),\n",
    "    StructField(\"transaction_time\", TimestampType(), nullable=False),\n",
    "    StructField(\"fraud_tag\", IntegerType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Create the sample data\n",
    "data = []\n",
    "for _ in range(2000):\n",
    "    last_email_change_days = random.randint(1, 30)\n",
    "    last_email_phone_number_change = random.randint(1, 10)\n",
    "    transaction_amount = round(random.uniform(10, 500), 2)\n",
    "    transaction_time = datetime.now() - timedelta(days=random.randint(1, 30))\n",
    "    fraud_tag = random.choice([0, 1])  # 0 for non-fraud, 1 for fraud\n",
    "    data.append((last_email_change_days, last_email_phone_number_change, transaction_amount, transaction_time, fraud_tag))\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Perform feature engineering\n",
    "indexer = StringIndexer(inputCol=\"fraud_tag\", outputCol=\"label\")\n",
    "indexedData = indexer.fit(df).transform(df)\n",
    "\n",
    "featureCols = [\"last_email_change_days\", \"last_email_phone_number_change\", \"transaction_amount\"]\n",
    "assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\n",
    "assembledData = assembler.transform(indexedData)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "trainData, testData = assembledData.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Perform sequential pattern mining using PrefixSpan\n",
    "sequences = assembledData.select(\"features\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "prefixSpan = PrefixSpan(minSupport=0.1, maxPatternLength=10)\n",
    "prefixSpanModel = prefixSpan.fit(sequences)\n",
    "frequentPatterns = prefixSpanModel.freqSequences\n",
    "\n",
    "# Build and train the LSTM model\n",
    "lstm = LSTMClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    maxIter=10,\n",
    "    stepSize=0.1,\n",
    "    blockSize=128,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[lstm])\n",
    "\n",
    "model = pipeline.fit(trainData)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(\"Area Under ROC Curve (AUC):\", auc)\n",
    "\n",
    "# Display frequent sequential patterns\n",
    "frequentPatterns.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9866a8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac8ca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, TimestampType, ArrayType\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LSTMClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.fpm import PrefixSpan\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"AccountFraudDetection\").getOrCreate()\n",
    "\n",
    "# Define the schema of the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"last_email_change_days\", IntegerType(), nullable=False),\n",
    "    StructField(\"last_email_phone_number_change\", IntegerType(), nullable=False),\n",
    "    StructField(\"transaction_amount\", DoubleType(), nullable=False),\n",
    "    StructField(\"transaction_time\", TimestampType(), nullable=False),\n",
    "    StructField(\"fraud_tag\", IntegerType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Create the sample data\n",
    "data = []\n",
    "for _ in range(2000):\n",
    "    last_email_change_days = random.randint(1, 30)\n",
    "    last_email_phone_number_change = random.randint(1, 10)\n",
    "    transaction_amount = round(random.uniform(10, 500), 2)\n",
    "    transaction_time = datetime.now() - timedelta(days=random.randint(1, 30))\n",
    "    fraud_tag = random.choice([0, 1])  # 0 for non-fraud, 1 for fraud\n",
    "    data.append((last_email_change_days, last_email_phone_number_change, transaction_amount, transaction_time, fraud_tag))\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Perform feature engineering\n",
    "indexer = StringIndexer(inputCol=\"fraud_tag\", outputCol=\"label\")\n",
    "indexedData = indexer.fit(df).transform(df)\n",
    "\n",
    "featureCols = [\"last_email_change_days\", \"last_email_phone_number_change\", \"transaction_amount\"]\n",
    "assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\n",
    "assembledData = assembler.transform(indexedData)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "trainData, testData = assembledData.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Perform sequential pattern mining using PrefixSpan\n",
    "sequences = assembledData.select(\"features\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "prefixSpan = PrefixSpan(minSupport=0.1, maxPatternLength=10)\n",
    "prefixSpanModel = prefixSpan.fit(sequences)\n",
    "frequentPatterns = prefixSpanModel.freqSequences\n",
    "\n",
    "# Convert frequentPatterns to a list of feature vectors\n",
    "patternFeatures = frequentPatterns.rdd.map(lambda row: row.sequence).collect()\n",
    "\n",
    "# Combine frequentPatterns features with existing features\n",
    "combinedFeatures = []\n",
    "for i in range(len(assembledData)):\n",
    "    existingFeatures = assembledData.select(\"features\").collect()[i][0]\n",
    "    combinedFeature = existingFeatures + patternFeatures[i]\n",
    "    combinedFeatures.append(combinedFeature)\n",
    "\n",
    "# Convert combinedFeatures back to DataFrame\n",
    "combinedData = spark.createDataFrame([(features,) for features in combinedFeatures], [\"combined_features\"])\n",
    "\n",
    "# Split the combined data into training and testing sets\n",
    "combinedTrainData, combinedTestData = combinedData.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Build and train the LSTM model with combined features\n",
    "lstm = LSTMClassifier(\n",
    "    featuresCol=\"combined_features\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    maxIter=10,\n",
    "    stepSize=0.1,\n",
    "    blockSize=128,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[lstm])\n",
    "\n",
    "model = pipeline.fit(combinedTrainData)\n",
    "\n",
    "# Make predictions on the combined test data\n",
    "combinedPredictions = model.transform(combinedTestData)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(combinedPredictions)\n",
    "print(\"Area Under ROC Curve (AUC):\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a240fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this updated code, after performing sequential pattern mining using PrefixSpan, the frequent patterns (frequentPatterns) are converted into a list of feature vectors (patternFeatures) by extracting the sequence column from the frequentPatterns DataFrame. Each pattern sequence is added to the corresponding existing features in the combinedFeatures list.\n",
    "\n",
    "Then, the combinedFeatures list is used to create a new DataFrame (combinedData) with a single column named \"combined_features\". This DataFrame is split into training and testing sets (combinedTrainData and combinedTestData).\n",
    "\n",
    "The LSTM model is built and trained using the combined features, where featuresCol is set to \"combined_features\".\n",
    "\n",
    "Finally, predictions are made on the combined test data, and the performance of the model is evaluated using the AUC metric.\n",
    "\n",
    "Please note that the sequential patterns obtained from PrefixSpan are appended to the existing features as additional dimensions. You may need to adjust the feature engineering step and handle any data type or dimensionality issues based on your specific dataset and use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd38132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b650b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c70cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe2c820",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ok Can you give me more methods for sequential pattern detection in pyspark and update the code again\n",
    "\n",
    "ChatGPT\n",
    "Certainly! In addition to the PrefixSpan algorithm, PySpark's MLlib library provides other methods for sequential pattern detection, such as FPGrowth and FPMax. Here's an updated code example that includes the FPGrowth algorithm for sequential pattern detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8b40e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, TimestampType, ArrayType\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LSTMClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"AccountFraudDetection\").getOrCreate()\n",
    "\n",
    "# Define the schema of the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"last_email_change_days\", IntegerType(), nullable=False),\n",
    "    StructField(\"last_email_phone_number_change\", IntegerType(), nullable=False),\n",
    "    StructField(\"transaction_amount\", DoubleType(), nullable=False),\n",
    "    StructField(\"transaction_time\", TimestampType(), nullable=False),\n",
    "    StructField(\"fraud_tag\", IntegerType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Create the sample data\n",
    "data = []\n",
    "for _ in range(2000):\n",
    "    last_email_change_days = random.randint(1, 30)\n",
    "    last_email_phone_number_change = random.randint(1, 10)\n",
    "    transaction_amount = round(random.uniform(10, 500), 2)\n",
    "    transaction_time = datetime.now() - timedelta(days=random.randint(1, 30))\n",
    "    fraud_tag = random.choice([0, 1])  # 0 for non-fraud, 1 for fraud\n",
    "    data.append((last_email_change_days, last_email_phone_number_change, transaction_amount, transaction_time, fraud_tag))\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Perform feature engineering\n",
    "indexer = StringIndexer(inputCol=\"fraud_tag\", outputCol=\"label\")\n",
    "indexedData = indexer.fit(df).transform(df)\n",
    "\n",
    "featureCols = [\"last_email_change_days\", \"last_email_phone_number_change\", \"transaction_amount\"]\n",
    "assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\n",
    "assembledData = assembler.transform(indexedData)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "trainData, testData = assembledData.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Perform sequential pattern mining using FPGrowth\n",
    "sequences = assembledData.select(\"features\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "fpgrowth = FPGrowth(itemsCol=\"features\", minSupport=0.1, minConfidence=0.5)\n",
    "fpgrowthModel = fpgrowth.fit(assembledData)\n",
    "frequentPatterns = fpgrowthModel.freqItemsets\n",
    "\n",
    "# Convert frequentPatterns to a list of feature vectors\n",
    "patternFeatures = frequentPatterns.rdd.map(lambda row: row.items).collect()\n",
    "\n",
    "# Combine frequentPatterns features with existing features\n",
    "combinedFeatures = []\n",
    "for i in range(len(assembledData)):\n",
    "    existingFeatures = assembledData.select(\"features\").collect()[i][0]\n",
    "    combinedFeature = existingFeatures + patternFeatures[i]\n",
    "    combinedFeatures.append(combinedFeature)\n",
    "\n",
    "# Convert combinedFeatures back to DataFrame\n",
    "combinedData = spark.createDataFrame([(features,) for features in combinedFeatures], [\"combined_features\"])\n",
    "\n",
    "# Split the combined data into training and testing sets\n",
    "combinedTrainData, combinedTestData = combinedData.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Build and train the LSTM model with combined features\n",
    "lstm = LSTMClassifier(\n",
    "    featuresCol=\"combined_features\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    maxIter=10,\n",
    "    stepSize=0.1,\n",
    "    blockSize=128,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[lstm])\n",
    "\n",
    "model = pipeline.fit(combinedTrainData)\n",
    "\n",
    "# Make predictions on the combined test data\n",
    "combinedPredictions = model.transform(combinedTestData)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(combinedPredictions)\n",
    "print(\"Area Under ROC Curve (AUC):\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dcf3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this updated code, the FPGrowth algorithm is used for sequential pattern mining instead of the PrefixSpan algorithm.\n",
    "\n",
    "After performing sequential pattern mining using FPGrowth, the frequent patterns (frequentPatterns) are converted into a list of feature vectors (patternFeatures) by extracting the items column from the frequentPatterns DataFrame.\n",
    "\n",
    "The subsequent steps for combining the frequent patterns with the existing features and training the LSTM model remain the same as before.\n",
    "\n",
    "Please note that you may need to adjust the parameters of the FPGrowth algorithm, such as minSupport and minConfidence, based on your specific dataset and requirements.\n",
    "\n",
    "Remember to adapt the code according to your specific use case and dataset characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc46ff0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f825df8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f25b19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9929a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Where to use pcaAssembledData into LSTM as feature. ?\n",
    "\n",
    "ChatGPT\n",
    "Apologies for the confusion in the previous response. In the updated code, the pcaAssembledData is not used directly in the LSTM model as a feature. Instead, the pcaAssembledData is used to transform the combinedTrainData and combinedTestData DataFrames by adding the PCA features as a new column (pcaVector). The transformed DataFrames with PCA features are then used as input for training and making predictions with the LSTM model.\n",
    "\n",
    "Here's the modified code that uses pcaAssembledData to transform the training and testing datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51a01e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, TimestampType, ArrayType\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LSTMClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.fpm import PrefixSpan\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"AccountFraudDetection\").getOrCreate()\n",
    "\n",
    "# Define the schema of the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"last_email_change_days\", IntegerType(), nullable=False),\n",
    "    StructField(\"last_email_phone_number_change\", IntegerType(), nullable=False),\n",
    "    StructField(\"transaction_amount\", DoubleType(), nullable=False),\n",
    "    StructField(\"transaction_time\", TimestampType(), nullable=False),\n",
    "    StructField(\"fraud_tag\", IntegerType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Create the sample data\n",
    "data = []\n",
    "for _ in range(2000):\n",
    "    last_email_change_days = random.randint(1, 30)\n",
    "    last_email_phone_number_change = random.randint(1, 10)\n",
    "    transaction_amount = round(random.uniform(10, 500), 2)\n",
    "    transaction_time = datetime.now() - timedelta(days=random.randint(1, 30))\n",
    "    fraud_tag = random.choice([0, 1])  # 0 for non-fraud, 1 for fraud\n",
    "    data.append((last_email_change_days, last_email_phone_number_change, transaction_amount, transaction_time, fraud_tag))\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Perform feature engineering\n",
    "indexer = StringIndexer(inputCol=\"fraud_tag\", outputCol=\"label\")\n",
    "indexedData = indexer.fit(df).transform(df)\n",
    "\n",
    "featureCols = [\"last_email_change_days\", \"last_email_phone_number_change\", \"transaction_amount\"]\n",
    "assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\n",
    "assembledData = assembler.transform(indexedData)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "trainData, testData = assembledData.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Perform sequential pattern mining using PrefixSpan\n",
    "sequences = assembledData.select(\"features\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "prefixSpan = PrefixSpan(minSupport=0.1, maxPatternLength=10)\n",
    "prefixSpanModel = prefixSpan.fit(sequences)\n",
    "frequentPatterns = prefixSpanModel.freqSequences\n",
    "\n",
    "# Convert frequentPatterns to a list of feature vectors\n",
    "patternFeatures = frequentPatterns.rdd.map(lambda row: row.sequence).collect()\n",
    "\n",
    "# Combine frequentPatterns features with existing features\n",
    "combinedFeatures = []\n",
    "for i in range(len(assembledData)):\n",
    "    existingFeatures = assembledData.select(\"features\").collect()[i][0]\n",
    "    combinedFeature = existingFeatures + patternFeatures[i]\n",
    "    combinedFeatures.append(combinedFeature)\n",
    "\n",
    "# Convert combinedFeatures back to DataFrame\n",
    "combinedData = spark.createDataFrame([(features,) for features in combinedFeatures], [\"combined_features\"])\n",
    "\n",
    "# Split the combined data into training and testing sets\n",
    "combinedTrainData, combinedTestData = combinedData.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca = PCA(k=2, inputCol=\"combined_features\", outputCol=\"pcaFeatures\")\n",
    "pcaModel = pca.fit(combinedData)\n",
    "pcaData = pcaModel.transform(combinedData)\n",
    "\n",
    "# Extract PCA features as input for LSTM\n",
    "pcaAssembler = VectorAssembler(inputCols=[\"pcaFeatures\"], outputCol=\"pcaVector\")\n",
    "pcaAssembledData = pcaAssembler.transform(pcaData)\n",
    "\n",
    "# Extract PCA-transformed features for training data\n",
    "pcaTrainData = pcaAssembledData.select(\"pcaVector\").join(combinedTrainData)\n",
    "\n",
    "# Extract PCA-transformed features for test data\n",
    "pcaTestData = pcaAssembledData.select(\"pcaVector\").join(combinedTestData)\n",
    "\n",
    "# Build and train the LSTM model with combined features\n",
    "lstm = LSTMClassifier(\n",
    "    featuresCol=\"pcaVector\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    maxIter=10,\n",
    "    stepSize=0.1,\n",
    "    blockSize=128,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[lstm])\n",
    "\n",
    "model = pipeline.fit(pcaTrainData)\n",
    "\n",
    "# Make predictions on the combined test data\n",
    "combinedPredictions = model.transform(pcaTestData)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(combinedPredictions)\n",
    "print(\"Area Under ROC Curve (AUC):\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906db28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this modified code, after transforming the combinedData with PCA, the PCA features (pcaVector) are extracted using a VectorAssembler. The pcaAssembledData is then joined with the combinedTrainData and combinedTestData DataFrames to include the PCA-transformed features for training and testing.\n",
    "\n",
    "The LSTM model is built and trained using the \"pcaVector\" feature column, and predictions are made on the test data as before.\n",
    "\n",
    "Please note that the PCA features extracted from pcaAssembledData are used in the LSTM model in this code example. Remember to adjust the number of PCA components (k parameter) and adapt the code according to your specific dataset and requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f4900f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c97f5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80dee04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6425e781",
   "metadata": {},
   "outputs": [],
   "source": [
    "how to use features from both random forest and prefixSpanModel feature into LSTM and create final model from it.\n",
    "\n",
    "ChatGPT\n",
    "To use features from both the Random Forest model and the PrefixSpanModel in the LSTM model, you can combine the feature vectors from both models and create a new DataFrame with these combined features. Here's an updated code example that demonstrates this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2771a447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, TimestampType, ArrayType\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier, LSTMClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.fpm import PrefixSpan\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"AccountFraudDetection\").getOrCreate()\n",
    "\n",
    "# Define the schema of the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"last_email_change_days\", IntegerType(), nullable=False),\n",
    "    StructField(\"last_email_phone_number_change\", IntegerType(), nullable=False),\n",
    "    StructField(\"transaction_amount\", DoubleType(), nullable=False),\n",
    "    StructField(\"transaction_time\", TimestampType(), nullable=False),\n",
    "    StructField(\"fraud_tag\", IntegerType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Create the sample data\n",
    "data = []\n",
    "for _ in range(2000):\n",
    "    last_email_change_days = random.randint(1, 30)\n",
    "    last_email_phone_number_change = random.randint(1, 10)\n",
    "    transaction_amount = round(random.uniform(10, 500), 2)\n",
    "    transaction_time = datetime.now() - timedelta(days=random.randint(1, 30))\n",
    "    fraud_tag = random.choice([0, 1])  # 0 for non-fraud, 1 for fraud\n",
    "    data.append((last_email_change_days, last_email_phone_number_change, transaction_amount, transaction_time, fraud_tag))\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Perform feature engineering\n",
    "indexer = StringIndexer(inputCol=\"fraud_tag\", outputCol=\"label\")\n",
    "indexedData = indexer.fit(df).transform(df)\n",
    "\n",
    "featureCols = [\"last_email_change_days\", \"last_email_phone_number_change\", \"transaction_amount\"]\n",
    "assembler = VectorAssembler(inputCols=featureCols, outputCol=\"original_features\")\n",
    "assembledData = assembler.transform(indexedData)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "trainData, testData = assembledData.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Perform sequential pattern mining using PrefixSpan\n",
    "sequences = assembledData.select(\"original_features\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "prefixSpan = PrefixSpan(minSupport=0.1, maxPatternLength=10)\n",
    "prefixSpanModel = prefixSpan.fit(sequences)\n",
    "frequentPatterns = prefixSpanModel.freqSequences\n",
    "\n",
    "# Convert frequentPatterns to a list of feature vectors\n",
    "patternFeatures = frequentPatterns.rdd.map(lambda row: row.sequence).collect()\n",
    "\n",
    "# Combine frequentPatterns features with original features\n",
    "combinedFeatures = []\n",
    "for i in range(len(assembledData)):\n",
    "    existingFeatures = assembledData.select(\"original_features\").collect()[i][0]\n",
    "    combinedFeature = existingFeatures + patternFeatures[i]\n",
    "    combinedFeatures.append(combinedFeature)\n",
    "\n",
    "# Convert combinedFeatures back to DataFrame\n",
    "combinedData = spark.createDataFrame([(features,) for features in combinedFeatures], [\"combined_features\"])\n",
    "\n",
    "# Split the combined data into training and testing sets\n",
    "combinedTrainData, combinedTestData = combinedData.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Build and train the Random Forest model with original features\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"original_features\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"rf_prediction\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pipelineRF = Pipeline(stages=[rf])\n",
    "\n",
    "rfModel = pipelineRF.fit(trainData)\n",
    "\n",
    "# Make predictions on the combined test data using Random Forest\n",
    "rfPredictions = rfModel.transform(testData)\n",
    "\n",
    "# Extract Random Forest features\n",
    "rfFeatures = rfPredictions.select(\"original_features\", \"rf_prediction\").rdd \\\n",
    "    .map(lambda row: (row[\"original_features\"], row[\"rf_prediction\"])).collect()\n",
    "\n",
    "# Combine Random Forest features with combined features\n",
    "combinedFeaturesRF = []\n",
    "for i in range(len(combinedFeatures)):\n",
    "    originalFeatures = combinedFeatures[i]\n",
    "    rfPrediction = rfFeatures[i][1]\n",
    "    combinedFeatureRF = originalFeatures + [rfPrediction]\n",
    "    combinedFeaturesRF.append(combinedFeatureRF)\n",
    "\n",
    "# Convert combinedFeaturesRF back to DataFrame\n",
    "combinedDataRF = spark.createDataFrame([(features,) for features in combinedFeaturesRF], [\"combined_features_rf\"])\n",
    "\n",
    "# Split the combined data into training and testing sets\n",
    "combinedTrainDataRF, combinedTestDataRF = combinedDataRF.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Build and train the LSTM model with combined features\n",
    "lstm = LSTMClassifier(\n",
    "    featuresCol=\"combined_features_rf\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"lstm_prediction\",\n",
    "    maxIter=10,\n",
    "    stepSize=0.1,\n",
    "    blockSize=128,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pipelineLSTM = Pipeline(stages=[lstm])\n",
    "\n",
    "modelLSTM = pipelineLSTM.fit(combinedTrainDataRF)\n",
    "\n",
    "# Make predictions on the combined test data using LSTM\n",
    "lstmPredictions = modelLSTM.transform(combinedTestDataRF)\n",
    "\n",
    "# Evaluate the LSTM model's performance\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"lstm_prediction\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(lstmPredictions)\n",
    "print(\"LSTM - Area Under ROC Curve (AUC):\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb583ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this updated code, after obtaining the features from the Random Forest model (rfFeatures), they are combined with the features from the PrefixSpan model (combinedFeatures) to create combinedFeaturesRF.\n",
    "\n",
    "The combinedFeaturesRF is then converted back to a DataFrame (combinedDataRF), and the LSTM model is trained on combinedTrainDataRF and evaluated on combinedTestDataRF.\n",
    "\n",
    "Please note that this code assumes that the Random Forest predictions are stored in a column named \"rf_prediction\". You might need to adjust this column name based on the structure of your Random Forest model's predictions.\n",
    "\n",
    "Remember to customize the code according to your specific use case and dataset characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37415369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b53c49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
