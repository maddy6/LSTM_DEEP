{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1b9ec34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   amount  account_number  address_change  email_id_change  phone_change  \\\n",
      "0     131            7726               0                1             1   \n",
      "1     179           11795              14               19            23   \n",
      "2      82            3340               0                1             0   \n",
      "3     891            4970               0                0             1   \n",
      "4     556            4943               0                1             1   \n",
      "\n",
      "   pin_change  fraud  \n",
      "0           0    0.0  \n",
      "1           1    1.0  \n",
      "2           0    0.0  \n",
      "3           0    0.0  \n",
      "4           1    0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set the number of fraud samples and total samples\n",
    "Total_samples = 20000\n",
    "Fraud_rate = 0.02\n",
    "Fraud_samples = int(Fraud_rate * Total_samples)\n",
    "\n",
    "# Generate fraud samples\n",
    "fraud_data = pd.DataFrame({'amount': np.random.randint(10, 1001, size=Fraud_samples),\n",
    "                           'account_number': np.random.randint(10000, 20000, size=Fraud_samples),\n",
    "                           'address_change': np.random.randint(0, 20, size=Fraud_samples),\n",
    "                           'email_id_change': np.random.randint(0, 40, size=Fraud_samples),\n",
    "                           'phone_change': np.random.randint(0, 30, size=Fraud_samples),\n",
    "                           'pin_change': np.random.randint(0, 3, size=Fraud_samples),\n",
    "                           'fraud': np.ones(Fraud_samples)})\n",
    "\n",
    "# Generate non-fraud samples\n",
    "non_fraud_samples = Total_samples - Fraud_samples\n",
    "non_fraud_data = pd.DataFrame({'amount': np.random.randint(10, 1001, size=non_fraud_samples),\n",
    "                               'account_number': np.random.randint(1000, 10000, size=non_fraud_samples),\n",
    "                               'address_change': np.random.randint(0, 2, size=non_fraud_samples),\n",
    "                               'email_id_change': np.random.randint(0, 2, size=non_fraud_samples),\n",
    "                               'phone_change': np.random.randint(0, 2, size=non_fraud_samples),\n",
    "                               'pin_change': np.random.randint(0, 2, size=non_fraud_samples),\n",
    "                               'fraud': np.zeros(non_fraud_samples)})\n",
    "\n",
    "# Concatenate fraud and non-fraud samples\n",
    "dataset = pd.concat([fraud_data, non_fraud_data], ignore_index=True)\n",
    "\n",
    "# Shuffle the dataset\n",
    "df = dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Preview the dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89dd0ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    19600\n",
       "1.0      400\n",
       "Name: fraud, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['fraud'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "800105d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "500/500 [==============================] - 5s 4ms/step - loss: 0.1312 - accuracy: 0.9923\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.0068 - accuracy: 0.9994\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.0029 - accuracy: 0.9997\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.0018 - accuracy: 0.9998\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 9.7851e-04 - accuracy: 0.9999\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 8.9317e-04 - accuracy: 0.9999\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 8.4509e-04 - accuracy: 0.9999\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 8.1228e-04 - accuracy: 0.9999\n",
      "125/125 [==============================] - 1s 3ms/step - loss: 1.2832e-04 - accuracy: 1.0000\n",
      "Loss: 0.00012831912317778915\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "\n",
    "# Load the dataset\n",
    "dataset = df  # Replace 'fraud_dataset.csv' with your dataset file name\n",
    "\n",
    "# Separate the features (X) and target variable (y)\n",
    "X = dataset.drop('fraud', axis=1)\n",
    "y = dataset['fraud']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the numerical features\n",
    "scaler = MinMaxScaler()\n",
    "X_train[['amount', 'account_number']] = scaler.fit_transform(X_train[['amount', 'account_number']])\n",
    "X_test[['amount', 'account_number']] = scaler.transform(X_test[['amount', 'account_number']])\n",
    "\n",
    "# Reshape the data for LSTM\n",
    "X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(1, X_train.shape[2])))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12df6e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[5.16161616e-01, 1.24432957e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.00000000e+00, 1.00000000e+00]],\n",
       "\n",
       "       [[4.38383838e-01, 7.86528115e-01, 1.30000000e+01, 2.00000000e+00,\n",
       "         3.00000000e+00, 1.00000000e+00]],\n",
       "\n",
       "       [[2.72727273e-02, 7.64848613e-03, 0.00000000e+00, 1.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2.17171717e-01, 2.72813588e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "         0.00000000e+00, 1.00000000e+00]],\n",
       "\n",
       "       [[6.70707071e-01, 2.88268805e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "       [[3.31313131e-01, 4.43612195e-02, 0.00000000e+00, 1.00000000e+00,\n",
       "         1.00000000e+00, 1.00000000e+00]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fd5eec",
   "metadata": {},
   "source": [
    "# Certainly! If you want to discover sequential patterns in the data to predict fraud, you can use a different approach called sequential pattern mining. This technique aims to extract frequent sequential patterns from the dataset.\n",
    "\n",
    "Here's a high-level overview of how you can apply sequential pattern mining for fraud detection:\n",
    "\n",
    "Preprocess the data: Convert categorical variables into numerical representations and normalize numerical features if necessary.\n",
    "\n",
    "Define the sequence representation: Map each record into a sequence of events. In your case, you can consider the \"fraud\" variable as the event of interest.\n",
    "\n",
    "Discover frequent sequential patterns: Use a sequential pattern mining algorithm, such as the AprioriAll algorithm or the PrefixSpan algorithm, to extract frequent sequential patterns from the dataset.\n",
    "\n",
    "Feature extraction: Extract features from the frequent sequential patterns to represent each record. For example, you can count the occurrences of specific patterns within a record or calculate statistics such as the average length of fraudulent sequences.\n",
    "\n",
    "Split the dataset: Divide the dataset into training and testing sets.\n",
    "\n",
    "Build and train a classification model: Use the extracted features as input to a classification model, such as a decision tree, random forest, or neural network, to predict fraud/non-fraud.\n",
    "\n",
    "Evaluate the model: Assess the performance of the model on the testing set using appropriate evaluation metrics, such as accuracy, precision, recall, and F1 score.\n",
    "\n",
    "It's worth noting that sequential pattern mining can be computationally intensive, especially for large datasets. You may need to adjust the parameters and algorithms based on your dataset size and specific requirements.\n",
    "\n",
    "Implementing sequential pattern mining for fraud detection requires more complex code and algorithms. If you'd like a more detailed example or assistance with implementing sequential pattern mining in Python, please let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173a1c80",
   "metadata": {},
   "source": [
    "# If you don't have access to the prefixspan library for sequential pattern mining, an alternative approach is to use the FP-growth algorithm, which is another popular algorithm for mining frequent itemsets and sequential patterns. The mlxtend library in Python provides an implementation of the FP-growth algorithm that you can use.\n",
    "\n",
    "Here's an example of how to use the FP-growth algorithm from the mlxtend library to discover frequent sequential patterns:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db49e48",
   "metadata": {},
   "source": [
    "# In the code above, we first convert the dataset into a list of transactions, where each transaction represents a sequence of events. Then, we one-hot encode the transactions using the TransactionEncoder from the mlxtend.preprocessing module. Finally, we apply the FP-growth algorithm on the one-hot encoded data using the fpgrowth function from the mlxtend.frequent_patterns module.\n",
    "\n",
    "Please note that you may need to install the mlxtend library by running pip install mlxtend in your Python environment.\n",
    "\n",
    "After discovering the frequent sequential patterns, you can proceed with the remaining steps, such as feature extraction, splitting the dataset, building and training a classification model, and evaluating the model as mentioned in the previous response.\n",
    "\n",
    "Let me know if you have any further questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c507db02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.49625</td>\n",
       "      <td>(email_id_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.48560</td>\n",
       "      <td>(phone_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.49685</td>\n",
       "      <td>(pin_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.49125</td>\n",
       "      <td>(address_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.24800</td>\n",
       "      <td>(pin_change, email_id_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.24755</td>\n",
       "      <td>(email_id_change, phone_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.24695</td>\n",
       "      <td>(pin_change, phone_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.23860</td>\n",
       "      <td>(address_change, phone_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.12620</td>\n",
       "      <td>(pin_change, email_id_change, phone_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.12335</td>\n",
       "      <td>(address_change, email_id_change, phone_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.12045</td>\n",
       "      <td>(address_change, pin_change, phone_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.24925</td>\n",
       "      <td>(address_change, email_id_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.24695</td>\n",
       "      <td>(address_change, pin_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.12640</td>\n",
       "      <td>(address_change, email_id_change, pin_change)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    support                                         itemsets\n",
       "0   0.49625                                (email_id_change)\n",
       "1   0.48560                                   (phone_change)\n",
       "2   0.49685                                     (pin_change)\n",
       "3   0.49125                                 (address_change)\n",
       "4   0.24800                    (pin_change, email_id_change)\n",
       "5   0.24755                  (email_id_change, phone_change)\n",
       "6   0.24695                       (pin_change, phone_change)\n",
       "7   0.23860                   (address_change, phone_change)\n",
       "8   0.12620      (pin_change, email_id_change, phone_change)\n",
       "9   0.12335  (address_change, email_id_change, phone_change)\n",
       "10  0.12045       (address_change, pin_change, phone_change)\n",
       "11  0.24925                (address_change, email_id_change)\n",
       "12  0.24695                     (address_change, pin_change)\n",
       "13  0.12640    (address_change, email_id_change, pin_change)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "\n",
    "# Load the dataset\n",
    "dataset = df  # Replace 'fraud_dataset.csv' with your dataset file name\n",
    "\n",
    "# Separate the features (X) and target variable (y)\n",
    "X = dataset.drop('fraud', axis=1)\n",
    "y = dataset['fraud']\n",
    "\n",
    "# Normalize the numerical features\n",
    "scaler = MinMaxScaler()\n",
    "X[['amount', 'account_number']] = scaler.fit_transform(X[['amount', 'account_number']])\n",
    "\n",
    "# Convert the dataset into a list of transactions\n",
    "transactions = X.apply(lambda row: [column for column, value in row.items() if value == 1], axis=1).tolist()\n",
    "\n",
    "# One-hot encode the transactions\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Discover frequent sequential patterns using FP-growth\n",
    "min_support = 0.1  # Adjust the minimum support threshold as per your dataset\n",
    "frequent_patterns = fpgrowth(df_encoded, min_support=min_support, use_colnames=True)\n",
    "frequent_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56b4db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07f049e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   amount  account_number  address_change  email_id_change  phone_change  \\\n",
      "0     574            9924               0                0             0   \n",
      "1     691            9335               0                0             0   \n",
      "\n",
      "   pin_change  fraud  \n",
      "0           0    0.0  \n",
      "1           0    0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set the number of fraud samples and total samples\n",
    "Total_samples = 200000\n",
    "Fraud_rate = 0.02\n",
    "Fraud_samples = int(Fraud_rate * Total_samples)\n",
    "\n",
    "# Generate fraud samples\n",
    "fraud_data = pd.DataFrame({'amount': np.random.randint(10, 1001, size=Fraud_samples),\n",
    "                           'account_number': np.random.randint(10000, 20000, size=Fraud_samples),\n",
    "                           'address_change': np.random.randint(0, 1, size=Fraud_samples),\n",
    "                           'email_id_change': np.random.randint(0, 1, size=Fraud_samples),\n",
    "                           'phone_change': np.random.randint(0, 1, size=Fraud_samples),\n",
    "                           'pin_change': np.random.randint(0, 1, size=Fraud_samples),\n",
    "                           'fraud': np.ones(Fraud_samples)})\n",
    "\n",
    "# Generate non-fraud samples\n",
    "non_fraud_samples = Total_samples - Fraud_samples\n",
    "non_fraud_data = pd.DataFrame({'amount': np.random.randint(10, 1001, size=non_fraud_samples),\n",
    "                               'account_number': np.random.randint(1000, 10000, size=non_fraud_samples),\n",
    "                               'address_change': np.random.randint(0, 1, size=non_fraud_samples),\n",
    "                               'email_id_change': np.random.randint(0, 1, size=non_fraud_samples),\n",
    "                               'phone_change': np.random.randint(0, 1, size=non_fraud_samples),\n",
    "                               'pin_change': np.random.randint(0, 1, size=non_fraud_samples),\n",
    "                               'fraud': np.zeros(non_fraud_samples)})\n",
    "\n",
    "# Concatenate fraud and non-fraud samples\n",
    "dataset = pd.concat([fraud_data, non_fraud_data], ignore_index=True)\n",
    "\n",
    "# Shuffle the dataset\n",
    "df = dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Preview the dataset\n",
    "print(df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "49fe82c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   amount  account_number  address_change  email_id_change  phone_change  \\\n",
      "0     305            8186             1.0              1.0           0.0   \n",
      "1     160            3113             0.0              0.0           0.0   \n",
      "2     654            4629             0.0              0.0           0.0   \n",
      "3     694            2778             0.0              0.0           0.0   \n",
      "4     512            3261             0.0              0.0           0.0   \n",
      "\n",
      "   pin_change  fraud  \n",
      "0         1.0    0.0  \n",
      "1         0.0    0.0  \n",
      "2         0.0    0.0  \n",
      "3         0.0    0.0  \n",
      "4         0.0    0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of records in the dataset\n",
    "num_records = 2000\n",
    "\n",
    "# Calculate the number of records for each variable\n",
    "num_address_change = int(num_records * 0.20)\n",
    "num_email_id_change = int(num_records * 0.10)\n",
    "num_phone_change = int(num_records * 0.05)\n",
    "num_pin_change = int(num_records * 0.07)\n",
    "\n",
    "# Calculate the number of fraud cases\n",
    "num_fraud = int(num_records * 0.0002)\n",
    "\n",
    "# Generate the dataset\n",
    "data = {\n",
    "    'amount': np.random.randint(10, 1001, num_records),\n",
    "    'account_number': np.random.randint(1000, 10000, num_records),\n",
    "    'address_change': np.concatenate((np.zeros(num_records - num_address_change), np.ones(num_address_change))),\n",
    "    'email_id_change': np.concatenate((np.zeros(num_records - num_email_id_change), np.ones(num_email_id_change))),\n",
    "    'phone_change': np.concatenate((np.zeros(num_records - num_phone_change), np.ones(num_phone_change))),\n",
    "    'pin_change': np.concatenate((np.zeros(num_records - num_pin_change), np.ones(num_pin_change))),\n",
    "    'fraud': np.concatenate((np.zeros(num_records - num_fraud), np.ones(num_fraud)))\n",
    "}\n",
    "\n",
    "# Shuffle the dataset\n",
    "df = pd.DataFrame(data).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display the dataset\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a600daee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    1800\n",
       "1.0     200\n",
       "Name: email_id_change, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['email_id_change'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e646f69b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f8a6871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pixel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\pixel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\pixel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "dataset = df  # Replace 'fraud_dataset.csv' with your dataset file name\n",
    "\n",
    "# Separate the features (X) and target variable (y)\n",
    "X = dataset.drop('fraud', axis=1)\n",
    "y = dataset['fraud']\n",
    "\n",
    "# Normalize the numerical features\n",
    "scaler = MinMaxScaler()\n",
    "X[['amount']] = scaler.fit_transform(X[['amount']])\n",
    "\n",
    "# Convert the dataset into a list of transactions\n",
    "transactions = X.apply(lambda row: [column for column, value in row.items() if value == 1], axis=1).tolist()\n",
    "\n",
    "# One-hot encode the transactions\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Discover frequent sequential patterns using FP-growth\n",
    "min_support = 0.1  # Adjust the minimum support threshold as per your dataset\n",
    "frequent_patterns = fpgrowth(df_encoded, min_support=min_support, use_colnames=True)\n",
    "\n",
    "# Feature extraction\n",
    "def extract_features(row):\n",
    "    features = []\n",
    "    for pattern in frequent_patterns['itemsets']:\n",
    "        count = 1 if pattern.issubset(row) else 0\n",
    "        features.append(count)\n",
    "    return features\n",
    "\n",
    "# Apply feature extraction to the dataset\n",
    "X_features = np.array(X.apply(extract_features, axis=1).tolist())\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build and train the classification model\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "007e1471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2</td>\n",
       "      <td>(address_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(email_id_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(address_change, email_id_change)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   support                           itemsets\n",
       "0      0.2                   (address_change)\n",
       "1      0.1                  (email_id_change)\n",
       "2      0.1  (address_change, email_id_change)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb9f3141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>account_number</th>\n",
       "      <th>address_change</th>\n",
       "      <th>email_id_change</th>\n",
       "      <th>phone_change</th>\n",
       "      <th>pin_change</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>305</td>\n",
       "      <td>8186</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160</td>\n",
       "      <td>3113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>654</td>\n",
       "      <td>4629</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>694</td>\n",
       "      <td>2778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>512</td>\n",
       "      <td>3261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   amount  account_number  address_change  email_id_change  phone_change  \\\n",
       "0     305            8186             1.0              1.0           0.0   \n",
       "1     160            3113             0.0              0.0           0.0   \n",
       "2     654            4629             0.0              0.0           0.0   \n",
       "3     694            2778             0.0              0.0           0.0   \n",
       "4     512            3261             0.0              0.0           0.0   \n",
       "\n",
       "   pin_change  fraud  \n",
       "0         1.0    0.0  \n",
       "1         0.0    0.0  \n",
       "2         0.0    0.0  \n",
       "3         0.0    0.0  \n",
       "4         0.0    0.0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f04e09d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    2000\n",
       "Name: fraud, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['fraud'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f796574b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f99df942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   amount  account_number  address_change  email_id_change  phone_change  \\\n",
      "0     294            2567             0.0              0.0           0.0   \n",
      "1     730            3166             0.0              0.0           0.0   \n",
      "2     903            8976             0.0              0.0           0.0   \n",
      "3     742            1862             0.0              0.0           0.0   \n",
      "4     754            6767             0.0              0.0           0.0   \n",
      "\n",
      "   pin_change  fraud  \n",
      "0         0.0    0.0  \n",
      "1         0.0    0.0  \n",
      "2         0.0    0.0  \n",
      "3         0.0    0.0  \n",
      "4         0.0    0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of records in the dataset\n",
    "num_records = 200000\n",
    "\n",
    "# Calculate the number of records for each variable\n",
    "num_address_change = int(num_records * 0.40)\n",
    "num_email_id_change = int(num_records * 0.30)\n",
    "num_phone_change = int(num_records * 0.2)\n",
    "num_pin_change = int(num_records * 0.15)\n",
    "\n",
    "# Calculate the number of fraud cases\n",
    "num_fraud = int(num_records * 0.03)\n",
    "\n",
    "# Generate the dataset\n",
    "data = {\n",
    "    'amount': np.random.randint(10, 1001, num_records),\n",
    "    'account_number': np.random.randint(1000, 10000, num_records),\n",
    "    'address_change': np.concatenate((np.zeros(num_records - num_address_change), np.ones(num_address_change))),\n",
    "    'email_id_change': np.concatenate((np.zeros(num_records - num_email_id_change), np.ones(num_email_id_change))),\n",
    "    'phone_change': np.concatenate((np.zeros(num_records - num_phone_change), np.ones(num_phone_change))),\n",
    "    'pin_change': np.concatenate((np.zeros(num_records - num_pin_change), np.ones(num_pin_change))),\n",
    "    'fraud': np.concatenate((np.zeros(num_records - num_fraud), np.ones(num_fraud)))\n",
    "}\n",
    "\n",
    "# Shuffle the dataset\n",
    "df = pd.DataFrame(data).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display the dataset\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "df46e6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    194000\n",
       "1.0      6000\n",
       "Name: fraud, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['fraud'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7877b38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pixel\\anaconda3\\lib\\random.py:370: DeprecationWarning: non-integer arguments to randrange() have been deprecated since Python 3.10 and will be removed in a subsequent version\n",
      "  return self.randrange(a, b+1)\n",
      "C:\\Users\\pixel\\anaconda3\\lib\\random.py:370: DeprecationWarning: non-integer arguments to randrange() have been deprecated since Python 3.10 and will be removed in a subsequent version\n",
      "  return self.randrange(a, b+1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pixel\\anaconda3\\lib\\random.py:370: DeprecationWarning: non-integer arguments to randrange() have been deprecated since Python 3.10 and will be removed in a subsequent version\n",
      "  return self.randrange(a, b+1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 67s 13ms/step - loss: 0.1376 - accuracy: 0.9698\n",
      "Epoch 2/10\n",
      "5000/5000 [==============================] - 63s 13ms/step - loss: 0.1356 - accuracy: 0.9698\n",
      "Epoch 3/10\n",
      "5000/5000 [==============================] - 63s 13ms/step - loss: 0.1356 - accuracy: 0.9698\n",
      "Epoch 4/10\n",
      "5000/5000 [==============================] - 70s 14ms/step - loss: 0.1355 - accuracy: 0.9698\n",
      "Epoch 5/10\n",
      "5000/5000 [==============================] - 64s 13ms/step - loss: 0.1355 - accuracy: 0.9698\n",
      "Epoch 6/10\n",
      "5000/5000 [==============================] - 65s 13ms/step - loss: 0.1368 - accuracy: 0.9693\n",
      "Epoch 7/10\n",
      "5000/5000 [==============================] - 67s 13ms/step - loss: 0.1355 - accuracy: 0.9698\n",
      "Epoch 8/10\n",
      "5000/5000 [==============================] - 65s 13ms/step - loss: 0.1355 - accuracy: 0.9698\n",
      "Epoch 9/10\n",
      "5000/5000 [==============================] - 63s 13ms/step - loss: 0.1356 - accuracy: 0.9698\n",
      "Epoch 10/10\n",
      "5000/5000 [==============================] - 67s 13ms/step - loss: 0.1355 - accuracy: 0.9698\n",
      "1250/1250 [==============================] - 11s 7ms/step\n",
      "Accuracy: 0.97065\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pixel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "dataset = df # Replace 'fraud_dataset.csv' with your dataset file name\n",
    "\n",
    "# Separate the features (X) and target variable (y)\n",
    "X = dataset.drop('fraud', axis=1)\n",
    "y = dataset['fraud']\n",
    "\n",
    "# Normalize the numerical features\n",
    "scaler = MinMaxScaler()\n",
    "X[['amount', 'account_number']] = scaler.fit_transform(X[['amount', 'account_number']])\n",
    "\n",
    "# Convert the dataset into a list of transactions\n",
    "transactions = X.apply(lambda row: [column for column, value in row.items() if value == 1], axis=1).tolist()\n",
    "\n",
    "# One-hot encode the transactions\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Discover frequent sequential patterns using FP-growth\n",
    "min_support = 0.1  # Adjust the minimum support threshold as per your dataset\n",
    "frequent_patterns = fpgrowth(df_encoded, min_support=min_support, use_colnames=True)\n",
    "\n",
    "# Feature extraction\n",
    "def extract_features(row):\n",
    "    features = []\n",
    "    for pattern in frequent_patterns['itemsets']:\n",
    "        count = 1 if pattern.issubset(row) else 0\n",
    "        features.append(count)\n",
    "    return features\n",
    "\n",
    "# Apply feature extraction to the dataset\n",
    "X_features = np.array(X.apply(extract_features, axis=1).tolist())\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the input variables for LSTM\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the LSTM model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.round(y_pred_prob).flatten()\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "954b0391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.40</td>\n",
       "      <td>(address_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.30</td>\n",
       "      <td>(email_id_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.20</td>\n",
       "      <td>(phone_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.15</td>\n",
       "      <td>(pin_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.30</td>\n",
       "      <td>(address_change, email_id_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.20</td>\n",
       "      <td>(address_change, phone_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.15</td>\n",
       "      <td>(address_change, pin_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.20</td>\n",
       "      <td>(email_id_change, phone_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.15</td>\n",
       "      <td>(pin_change, email_id_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.15</td>\n",
       "      <td>(pin_change, phone_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.20</td>\n",
       "      <td>(address_change, email_id_change, phone_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.15</td>\n",
       "      <td>(address_change, email_id_change, pin_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.15</td>\n",
       "      <td>(address_change, pin_change, phone_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.15</td>\n",
       "      <td>(pin_change, email_id_change, phone_change)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.15</td>\n",
       "      <td>(address_change, email_id_change, pin_change, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    support                                           itemsets\n",
       "0      0.40                                   (address_change)\n",
       "1      0.30                                  (email_id_change)\n",
       "2      0.20                                     (phone_change)\n",
       "3      0.15                                       (pin_change)\n",
       "4      0.30                  (address_change, email_id_change)\n",
       "5      0.20                     (address_change, phone_change)\n",
       "6      0.15                       (address_change, pin_change)\n",
       "7      0.20                    (email_id_change, phone_change)\n",
       "8      0.15                      (pin_change, email_id_change)\n",
       "9      0.15                         (pin_change, phone_change)\n",
       "10     0.20    (address_change, email_id_change, phone_change)\n",
       "11     0.15      (address_change, email_id_change, pin_change)\n",
       "12     0.15         (address_change, pin_change, phone_change)\n",
       "13     0.15        (pin_change, email_id_change, phone_change)\n",
       "14     0.15  (address_change, email_id_change, pin_change, ..."
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b2720615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a8e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e98cf597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119737    0.0\n",
       "72272     0.0\n",
       "158154    0.0\n",
       "65426     0.0\n",
       "30074     0.0\n",
       "         ... \n",
       "4174      0.0\n",
       "91537     0.0\n",
       "156449    0.0\n",
       "184376    0.0\n",
       "6584      0.0\n",
       "Name: fraud, Length: 40000, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "012c5ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    38826\n",
       "1.0     1174\n",
       "Name: fraud, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9123d329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323bc2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "78c0927f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "dataset = df  # Replace 'fraud_dataset.csv' with your dataset file name\n",
    "\n",
    "# Separate the features (X) and target variable (y)\n",
    "X = dataset.drop('fraud', axis=1)\n",
    "y = dataset['fraud']\n",
    "\n",
    "# Normalize the numerical features\n",
    "scaler = MinMaxScaler()\n",
    "X[['amount', 'account_number']] = scaler.fit_transform(X[['amount', 'account_number']])\n",
    "\n",
    "# Define the sliding window size\n",
    "window_size = 5  # Adjust the window size as per your requirement\n",
    "\n",
    "# Extract sequential pattern features using the sliding window approach\n",
    "def extract_features(data):\n",
    "    features = []\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        window = data[i:i + window_size]\n",
    "        pattern = tuple(window)\n",
    "        features.append(pattern)\n",
    "    return features\n",
    "\n",
    "# Apply feature extraction to the dataset\n",
    "X['sequential_pattern'] = X.apply(extract_features, axis=1)\n",
    "X_features = X['sequential_pattern'].tolist()\n",
    "\n",
    "# Convert sequential patterns into binary features\n",
    "unique_patterns = set(pattern for patterns in X_features for pattern in patterns)\n",
    "\n",
    "def pattern_to_binary(row):\n",
    "    features = []\n",
    "    for pattern in unique_patterns:\n",
    "        features.append(1 if pattern in row else 0)\n",
    "    return features\n",
    "\n",
    "X_features = np.array([pattern_to_binary(row) for row in X_features])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the input variables for LSTM\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the LSTM model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.round(y_pred_prob).flatten()\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f7a70dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff734d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "dataset = df  # Replace 'fraud_dataset.csv' with your dataset file name\n",
    "\n",
    "# Separate the features (X) and target variable (y)\n",
    "X = dataset.drop('fraud', axis=1)\n",
    "y = dataset['fraud']\n",
    "\n",
    "# Normalize the numerical features\n",
    "scaler = MinMaxScaler()\n",
    "X[['amount', 'account_number']] = scaler.fit_transform(X[['amount', 'account_number']])\n",
    "\n",
    "# Define the sliding window size\n",
    "window_size = 5  # Adjust the window size as per your requirement\n",
    "\n",
    "# Extract sequential pattern features using the sliding window approach\n",
    "def extract_features(data):\n",
    "    features = []\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        window = data[i:i + window_size]\n",
    "        pattern = tuple(window)\n",
    "        features.append(pattern)\n",
    "    return features\n",
    "\n",
    "# Apply feature extraction to the dataset\n",
    "X['sequential_pattern'] = X.apply(extract_features, axis=1)\n",
    "X_features = X['sequential_pattern'].tolist()\n",
    "\n",
    "# Convert sequential patterns into binary features\n",
    "unique_patterns = set(pattern for patterns in X_features for pattern in patterns)\n",
    "\n",
    "def pattern_to_binary(row):\n",
    "    features = []\n",
    "    for pattern in unique_patterns:\n",
    "        features.append(1 if pattern in row else 0)\n",
    "    return features\n",
    "\n",
    "X_features = np.array([pattern_to_binary(row) for row in X_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc30edc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1b6fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4381196a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a369fc28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
