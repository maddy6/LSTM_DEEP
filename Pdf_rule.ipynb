{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "793bdadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule 1:\n",
      "Features: Education, Age, Income\n",
      "Num Anomalies Captured: 30\n",
      "Total Rows Captured: 30\n",
      "Percentage Anomalies Captured: 100.00%\n",
      "Rule Condition: (Education == 'High') AND (Age > 0) AND (Income > 66500.0)\n",
      "\n",
      "Rule 2:\n",
      "Features: Gender, Age, Income\n",
      "Num Anomalies Captured: 30\n",
      "Total Rows Captured: 30\n",
      "Percentage Anomalies Captured: 100.00%\n",
      "Rule Condition: (Gender == 'Male') AND (Age > 0) AND (Income > 66500.0)\n",
      "\n",
      "Rule 3:\n",
      "Features: Education, Gender, Age, Income\n",
      "Num Anomalies Captured: 30\n",
      "Total Rows Captured: 30\n",
      "Percentage Anomalies Captured: 100.00%\n",
      "Rule Condition: (Education == 'High') AND (Gender == 'Male') AND (Age > 0) AND (Income > 66500.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "# Create the dataframe\n",
    "data = {\n",
    "    'Age': [25, 30, 22, 35, 28, 40, 45, 27, 33, 29] * 5,\n",
    "    'Income': [50000, 60000, 45000, 70000, 55000, 80000, 90000, 52000, 65000, 59000] * 5,\n",
    "    'Education': ['High', 'Low', 'Medium', 'Medium', 'High', 'Low', 'Medium', 'High', 'Low', 'Medium'] * 5,\n",
    "    'Gender': ['Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Female'] * 5,\n",
    "    'Anomaly': [False, True, False, True, True, False, True, False, True, True] * 5\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize variables\n",
    "all_rules = []\n",
    "max_rules = 20  # Generate more rules\n",
    "\n",
    "# Define which numerical features should be greater than while creating combinations\n",
    "numerical_features_greater = ['Income']\n",
    "\n",
    "# Generate combinations of features, including both numerical and categorical variables\n",
    "numerical_features = ['Age', 'Income']\n",
    "categorical_features = ['Education', 'Gender']\n",
    "\n",
    "# Helper function to generate all combinations of AND and OR conditions for categorical variables\n",
    "def generate_categorical_combinations(categories, variable_name):\n",
    "    combinations = []\n",
    "    # Generate all possible OR conditions with one unique value for each categorical variable\n",
    "    for category in categories:\n",
    "        combinations.append(f\"({variable_name} == '{category}')\")\n",
    "\n",
    "    # Generate all possible AND conditions by adding ' AND ' between OR conditions\n",
    "    and_condition = ' AND '.join(combinations)\n",
    "    combinations.append(and_condition)\n",
    "    return combinations\n",
    "\n",
    "# Initialize a dictionary to store the total number of rows captured by each rule\n",
    "rows_captured_by_rule = {}\n",
    "\n",
    "# Generate combinations of categorical features first\n",
    "for cat_features in range(1, len(categorical_features) + 1):\n",
    "    for num_features in itertools.combinations(categorical_features, cat_features):\n",
    "        feature_combination = list(num_features) + numerical_features\n",
    "\n",
    "        # Check if the combination is valid (Income should be greater)\n",
    "        if all(feature in feature_combination for feature in numerical_features_greater):\n",
    "            # Apply the combination of features\n",
    "            subset_df = df[feature_combination + ['Anomaly']]\n",
    "\n",
    "            # Filter the subset where the rule is satisfying (Anomaly is True)\n",
    "            satisfying_subset = subset_df[subset_df['Anomaly']]\n",
    "\n",
    "            # Calculate the number of anomalies in this subset\n",
    "            num_anomalies = satisfying_subset['Anomaly'].sum()\n",
    "\n",
    "            # Determine dynamic cutoffs for numerical features based on this subset\n",
    "            cutoffs = {}\n",
    "            for feature in numerical_features:\n",
    "                if feature in numerical_features_greater:\n",
    "                    # Use the average value where anomalies are present\n",
    "                    cutoff = satisfying_subset[feature].mean()\n",
    "                else:\n",
    "                    cutoff = 0  # Default cutoff for other numerical features\n",
    "                cutoffs[feature] = cutoff\n",
    "\n",
    "            # Generate rules for the current feature combination with dynamic cutoffs\n",
    "            current_rules = []\n",
    "            categorical_rule_added = set()  # To track which categorical variables have been added\n",
    "            for feature in feature_combination:\n",
    "                if feature in numerical_features:\n",
    "                    cutoff = cutoffs.get(feature, 0)  # Get dynamic cutoff or default to 0\n",
    "                    rule_condition = f\"({feature} > {cutoff})\"\n",
    "                else:\n",
    "                    variable_name = feature\n",
    "                    categories = df[feature].unique()\n",
    "                    if feature in categorical_features and variable_name not in categorical_rule_added:\n",
    "                        categorical_rule_added.add(variable_name)\n",
    "                        # Include only a single category in the rule\n",
    "                        rule_condition = f\"({variable_name} == '{categories[0]}')\"\n",
    "                    else:\n",
    "                        # Include all categories in the rule\n",
    "                        category_combinations = generate_categorical_combinations(categories, variable_name)\n",
    "                        rule_condition = ' AND '.join(category_combinations)\n",
    "                current_rules.append(rule_condition)\n",
    "\n",
    "            # Combine the rules into a single rule for the current feature combination\n",
    "            combined_rule = \" AND \".join(current_rules)\n",
    "\n",
    "            # Add the rule to the list\n",
    "            rule = {\n",
    "                'Features': ', '.join(feature_combination),\n",
    "                'Num Anomalies Captured': num_anomalies,\n",
    "                'Rule Condition': combined_rule,\n",
    "                'Total Rows Captured': len(satisfying_subset),\n",
    "                'Percentage Anomalies Captured': (num_anomalies / len(satisfying_subset)) * 100\n",
    "            }\n",
    "            all_rules.append(rule)\n",
    "\n",
    "            # Store the total number of rows captured by this rule\n",
    "            rows_captured_by_rule[combined_rule] = len(satisfying_subset)\n",
    "\n",
    "# Sort all the rules by the number of anomalies detected in descending order\n",
    "all_rules.sort(key=lambda x: x['Num Anomalies Captured'], reverse=True)\n",
    "\n",
    "# Print the top 20 rules with anomalies counts, total rows captured, and percentage anomalies captured\n",
    "for i, rule in enumerate(all_rules[:max_rules]):\n",
    "    num_anomalies = rule['Num Anomalies Captured']\n",
    "    rows_captured = rule['Total Rows Captured']\n",
    "    percentage_anomalies = rule['Percentage Anomalies Captured']\n",
    "    print(f\"Rule {i + 1}:\")\n",
    "    print(f\"Features: {rule['Features']}\")\n",
    "    print(f\"Num Anomalies Captured: {num_anomalies}\")\n",
    "    print(f\"Total Rows Captured: {rows_captured}\")\n",
    "    print(f\"Percentage Anomalies Captured: {percentage_anomalies:.2f}%\")\n",
    "    print(f\"Rule Condition: {rule['Rule Condition']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436c8680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80868780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pixel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pixel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: Education is High, Gender is Male.\n",
      "Most Similar Rule: Rule 1\n",
      "Matched Rule Text: (Education == 'High') AND (Gender == 'Male')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "\n",
    "# Define the input text\n",
    "text = \"Education is High, Gender is Male.\"\n",
    "\n",
    "# Define the list of anomaly rules\n",
    "Anomaly_rule_list = {\n",
    "    \"Rule 1\": \"(Education == 'High') AND (Gender == 'Male')\",\n",
    "    \"Rule 2\": \"(Education == 'High') AND (Age > 0) AND (Income > 66500.0)\",\n",
    "}\n",
    "\n",
    "# Tokenize the input text and remove stopwords\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "text_tokens = word_tokenize(text)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_text_tokens = [word for word in text_tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Create a dictionary to store the similarity scores for each rule\n",
    "rule_similarity = {}\n",
    "\n",
    "# Calculate Jaccard similarity between the input text and each rule\n",
    "for rule_name, rule_text in Anomaly_rule_list.items():\n",
    "    rule_tokens = word_tokenize(rule_text)\n",
    "    filtered_rule_tokens = [word for word in rule_tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Calculate Jaccard similarity\n",
    "    intersection = len(set(filtered_text_tokens).intersection(filtered_rule_tokens))\n",
    "    union = len(set(filtered_text_tokens).union(filtered_rule_tokens))\n",
    "    jaccard_similarity = intersection / union\n",
    "    \n",
    "    rule_similarity[rule_name] = jaccard_similarity\n",
    "\n",
    "# Find the rule with the highest similarity score\n",
    "most_similar_rule = max(rule_similarity, key=rule_similarity.get)\n",
    "\n",
    "# Get the corresponding rule text\n",
    "matched_rule_text = Anomaly_rule_list[most_similar_rule]\n",
    "\n",
    "print(f\"Input Text: {text}\")\n",
    "print(f\"Most Similar Rule: {most_similar_rule}\")\n",
    "print(f\"Matched Rule Text: {matched_rule_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cb5d45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd65f32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: Education is 'Low' , 'Medium' and Gender is male. Appendix...\n",
      "Age is 1 2 4 6 89. XYZ ltd. Among the largest university presses in the world,\n",
      "The MIT Press publishes over 200 new books each year along with 30 journals\n",
      "in the arts and humanities, economics, international affairs, history, political\n",
      "science, science and technology along with other disciplines. We were among\n",
      "the first university presses to offer titles electronically and we continue to\n",
      "adopt technologies that allow us to better support the scholarly mission and\n",
      "disseminate our content widely. The Press's enthusiasm for innovation is\n",
      "reflected in our continuing exploration of this frontier. Since the late 1960s,\n",
      "we have experimented with generation after generation of electronic publishing tools.\n",
      "Through our commitment to new products—whether digital journals or entirely\n",
      "new forms of communication—we have continued to look for the most efficient\n",
      "and effective means to serve our readership. Our readers have come to expect\n",
      "excellence from our products, and they can count on us to maintain a commitment\n",
      "to producing rigorous and innovative information products in whatever forms\n",
      "the future of publishing may bring. Education is High , Gender is Male.\n",
      "Most Similar Rule: Rule 1\n",
      "Matched Rule Text: (Education == 'High') AND (Gender == 'Male')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pixel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pixel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "\n",
    "# Define the input text\n",
    "text = \"\"\"Education is 'Low' , 'Medium' and Gender is male. Appendix...\n",
    "Age is 1 2 4 6 89. XYZ ltd. Among the largest university presses in the world,\n",
    "The MIT Press publishes over 200 new books each year along with 30 journals\n",
    "in the arts and humanities, economics, international affairs, history, political\n",
    "science, science and technology along with other disciplines. We were among\n",
    "the first university presses to offer titles electronically and we continue to\n",
    "adopt technologies that allow us to better support the scholarly mission and\n",
    "disseminate our content widely. The Press's enthusiasm for innovation is\n",
    "reflected in our continuing exploration of this frontier. Since the late 1960s,\n",
    "we have experimented with generation after generation of electronic publishing tools.\n",
    "Through our commitment to new products—whether digital journals or entirely\n",
    "new forms of communication—we have continued to look for the most efficient\n",
    "and effective means to serve our readership. Our readers have come to expect\n",
    "excellence from our products, and they can count on us to maintain a commitment\n",
    "to producing rigorous and innovative information products in whatever forms\n",
    "the future of publishing may bring. Education is High , Gender is Male.\"\"\"\n",
    "\n",
    "# Define the list of anomaly rules\n",
    "Anomaly_rule_list = {\n",
    "    \"Rule 1\": \"(Education == 'High') AND (Gender == 'Male')\",\n",
    "    \"Rule 2\": \"(Education == 'High') AND (Age > 0) AND (Income > 66500.0)\",\n",
    "}\n",
    "\n",
    "# Tokenize the input text and remove stopwords\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "text_tokens = word_tokenize(text)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_text_tokens = [word for word in text_tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Create a dictionary to store the similarity scores for each rule\n",
    "rule_similarity = {}\n",
    "\n",
    "# Calculate Jaccard similarity between the input text and each rule\n",
    "for rule_name, rule_text in Anomaly_rule_list.items():\n",
    "    rule_tokens = word_tokenize(rule_text)\n",
    "    filtered_rule_tokens = [word for word in rule_tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Calculate Jaccard similarity\n",
    "    intersection = len(set(filtered_text_tokens).intersection(filtered_rule_tokens))\n",
    "    union = len(set(filtered_text_tokens).union(filtered_rule_tokens))\n",
    "    jaccard_similarity = intersection / union\n",
    "    \n",
    "    rule_similarity[rule_name] = jaccard_similarity\n",
    "\n",
    "# Find the rule with the highest similarity score\n",
    "most_similar_rule = max(rule_similarity, key=rule_similarity.get)\n",
    "\n",
    "# Get the corresponding rule text\n",
    "matched_rule_text = Anomaly_rule_list[most_similar_rule]\n",
    "\n",
    "print(f\"Input Text: {text}\")\n",
    "print(f\"Most Similar Rule: {most_similar_rule}\")\n",
    "print(f\"Matched Rule Text: {matched_rule_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9aff31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21178e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23076923076923078\n",
      "0.11764705882352941\n",
      "0.0\n",
      "0.05263157894736842\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.15384615384615385\n",
      "0.125\n",
      "Rule 'Rule 1' Matches:\n",
      "- Education is 'Low' , 'Medium' and Gender is male.\n",
      "- Education is High , Gender is Male and age 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pixel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pixel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define the input text\n",
    "text = \"\"\"Education is 'Low' , 'Medium' and Gender is male. Appendix...\n",
    "Age is 1 2 4 6 89. XYZ ltd. Among the largest university presses in the world,\n",
    "The MIT Press publishes over 200 new books each year along with 30 journals\n",
    "in the arts and humanities, economics, international affairs, history, political\n",
    "science, science and technology along with other disciplines. We were among\n",
    "the first university presses to offer titles electronically and we continue to\n",
    "adopt technologies that allow us to better support the scholarly mission and\n",
    "disseminate our content widely. The Press's enthusiasm for innovation is\n",
    "reflected in our continuing exploration of this frontier. Since the late 1960s,\n",
    "we have experimented with generation after generation of electronic publishing tools.\n",
    "Through our commitment to new products—whether digital journals or entirely\n",
    "new forms of communication—we have continued to look for the most efficient\n",
    "and effective means to serve our readership. Our readers have come to expect\n",
    "excellence from our products, and they can count on us to maintain a commitment\n",
    "to producing rigorous and innovative information products in whatever forms\n",
    "the future of publishing may bring. Education is High , Gender is Male and age 0\"\"\"\n",
    "\n",
    "# Define the list of anomaly rules\n",
    "Anomaly_rule_list = {\n",
    "    \"Rule 1\": \"(Education == 'High') AND (Gender == 'Male')\",\n",
    "    \"Rule 2\": \"(Education == 'High') AND (Age > 0) AND (Income > 66500.0)\",\n",
    "}\n",
    "\n",
    "# Tokenize the input text and remove stopwords\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "text_sentences = sent_tokenize(text)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Create a dictionary to store the matched sentences for each rule\n",
    "matched_sentences = {}\n",
    "\n",
    "# Iterate through sentences and check if they match any rule\n",
    "for sentence in text_sentences:\n",
    "    sentence_tokens = word_tokenize(sentence)\n",
    "    filtered_sentence_tokens = [word for word in sentence_tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    for rule_name, rule_text in Anomaly_rule_list.items():\n",
    "        rule_tokens = word_tokenize(rule_text)\n",
    "        filtered_rule_tokens = [word for word in rule_tokens if word.lower() not in stop_words]\n",
    "        \n",
    "        # Calculate Jaccard similarity\n",
    "        intersection = len(set(filtered_sentence_tokens).intersection(filtered_rule_tokens))\n",
    "        union = len(set(filtered_sentence_tokens).union(filtered_rule_tokens))\n",
    "        jaccard_similarity = intersection / union\n",
    "        print(jaccard_similarity)\n",
    "        # If the similarity is above a threshold (e.g., 0.5), consider it a match\n",
    "        if jaccard_similarity > 0.15:\n",
    "            if rule_name in matched_sentences:\n",
    "                matched_sentences[rule_name].append(sentence)\n",
    "            else:\n",
    "                matched_sentences[rule_name] = [sentence]\n",
    "\n",
    "# Print matched sentences for each rule\n",
    "for rule_name, matched_sentence_list in matched_sentences.items():\n",
    "    print(f\"Rule '{rule_name}' Matches:\")\n",
    "    for sentence in matched_sentence_list:\n",
    "        print(f\"- {sentence}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83b7848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86adc838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Rule Matches:\n",
      "Rule 'Rule 1' Matches:\n",
      "- Education is 'Low' , 'Medium' and Gender is male.\n",
      "\n",
      "Partial Rule Matches:\n",
      "Rule 'Rule 2' Partial Matches:\n",
      "- Education is 'Low' , 'Medium' and Gender is male.\n",
      "- Education is High , Gender is Male age 0\n",
      "\n",
      "Rule 'Rule 1' Partial Matches:\n",
      "- Education is High , Gender is Male age 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pixel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pixel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define the input text\n",
    "text = \"\"\"Education is 'Low' , 'Medium' and Gender is male. Appendix...\n",
    "Age is 1 2 4 6 89. XYZ ltd. Among the largest university presses in the world,\n",
    "The MIT Press publishes over 200 new books each year along with 30 journals\n",
    "in the arts and humanities, economics, international affairs, history, political\n",
    "science, science and technology along with other disciplines. We were among\n",
    "the first university presses to offer titles electronically and we continue to\n",
    "adopt technologies that allow us to better support the scholarly mission and\n",
    "disseminate our content widely. The Press's enthusiasm for innovation is\n",
    "reflected in our continuing exploration of this frontier. Since the late 1960s,\n",
    "we have experimented with generation after generation of electronic publishing tools.\n",
    "Through our commitment to new products—whether digital journals or entirely\n",
    "new forms of communication—we have continued to look for the most efficient\n",
    "and effective means to serve our readership. Our readers have come to expect\n",
    "excellence from our products, and they can count on us to maintain a commitment\n",
    "to producing rigorous and innovative information products in whatever forms\n",
    "the future of publishing may bring. Education is High , Gender is Male age 0\"\"\"\n",
    "\n",
    "# Define the list of anomaly rules\n",
    "Anomaly_rule_list = {\n",
    "    \"Rule 1\": \"(Education == 'High') AND (Gender == 'Male')\",\n",
    "    \"Rule 2\": \"(Education == 'High') AND (Age > 0) AND (Income > 66500.0)\",\n",
    "}\n",
    "\n",
    "# Tokenize the input text and remove stopwords\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "text_sentences = sent_tokenize(text)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Create dictionaries to store the matched sentences and partial matches for each rule\n",
    "matched_sentences = {}\n",
    "partial_matches = {}\n",
    "\n",
    "# Iterate through sentences and check if they match any rule\n",
    "for sentence in text_sentences:\n",
    "    sentence_tokens = word_tokenize(sentence)\n",
    "    filtered_sentence_tokens = [word for word in sentence_tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    for rule_name, rule_text in Anomaly_rule_list.items():\n",
    "        rule_tokens = word_tokenize(rule_text)\n",
    "        filtered_rule_tokens = [word for word in rule_tokens if word.lower() not in stop_words]\n",
    "        \n",
    "        # Calculate Jaccard similarity\n",
    "        intersection = len(set(filtered_sentence_tokens).intersection(filtered_rule_tokens))\n",
    "        union = len(set(filtered_sentence_tokens).union(filtered_rule_tokens))\n",
    "        jaccard_similarity = intersection / union\n",
    "        #print(\"jaccard_similarity\", jaccard_similarity)\n",
    "        # If the similarity is above a threshold (e.g., 0.5), consider it a match\n",
    "        if jaccard_similarity > 0.06:\n",
    "            if jaccard_similarity >= 0.23:\n",
    "                if rule_name in matched_sentences:\n",
    "                    matched_sentences[rule_name].append(sentence)\n",
    "                else:\n",
    "                    matched_sentences[rule_name] = [sentence]\n",
    "            else:\n",
    "                if rule_name in partial_matches:\n",
    "                    partial_matches[rule_name].append(sentence)\n",
    "                else:\n",
    "                    partial_matches[rule_name] = [sentence]\n",
    "\n",
    "# Print exact rule matches\n",
    "print(\"Exact Rule Matches:\")\n",
    "for rule_name, matched_sentence_list in matched_sentences.items():\n",
    "    print(f\"Rule '{rule_name}' Matches:\")\n",
    "    for sentence in matched_sentence_list:\n",
    "        print(f\"- {sentence}\")\n",
    "    print()\n",
    "\n",
    "# Print partial rule matches\n",
    "print(\"Partial Rule Matches:\")\n",
    "for rule_name, partial_match_sentence_list in partial_matches.items():\n",
    "    print(f\"Rule '{rule_name}' Partial Matches:\")\n",
    "    for sentence in partial_match_sentence_list:\n",
    "        print(f\"- {sentence}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2d8864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4be0677a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Rule Matches:\n",
      "Rule 'Rule 1' Matches:\n",
      "- Education is 'Low' , 'Medium' and Gender is male.\n",
      "\n",
      "Partial Rule Matches:\n",
      "Rule 'Rule 2' Partial Matches:\n",
      "- Education is 'Low' , 'Medium' and Gender is male.\n",
      "- Education is High , Gender is Male age 0\n",
      "\n",
      "Rule 'Rule 1' Partial Matches:\n",
      "- Education is High , Gender is Male age 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define the input text\n",
    "text = \"\"\"Education is 'Low' , 'Medium' and Gender is male. Appendix...\n",
    "Age is 1 2 4 6 89. XYZ ltd. Among the largest university presses in the world,\n",
    "The MIT Press publishes over 200 new books each year along with 30 journals\n",
    "in the arts and humanities, economics, international affairs, history, political\n",
    "science, science and technology along with other disciplines. We were among\n",
    "the first university presses to offer titles electronically and we continue to\n",
    "adopt technologies that allow us to better support the scholarly mission and\n",
    "disseminate our content widely. The Press's enthusiasm for innovation is\n",
    "reflected in our continuing exploration of this frontier. Since the late 1960s,\n",
    "we have experimented with generation after generation of electronic publishing tools.\n",
    "Through our commitment to new products—whether digital journals or entirely\n",
    "new forms of communication—we have continued to look for the most efficient\n",
    "and effective means to serve our readership. Our readers have come to expect\n",
    "excellence from our products, and they can count on us to maintain a commitment\n",
    "to producing rigorous and innovative information products in whatever forms\n",
    "the future of publishing may bring. Education is High , Gender is Male age 0\"\"\"\n",
    "\n",
    "# Define the list of anomaly rules\n",
    "Anomaly_rule_list = {\n",
    "    \"Rule 1\": \"(Education == 'High') AND (Gender == 'Male')\",\n",
    "    \"Rule 2\": \"(Education == 'High') AND (Age > 0) AND (Income > 66500.0)\",\n",
    "}\n",
    "\n",
    "# Tokenize the input text and remove stopwords\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"stopwords\")\n",
    "\n",
    "text_sentences = sent_tokenize(text)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Create dictionaries to store the matched sentences and partial matches for each rule\n",
    "matched_sentences = {}\n",
    "partial_matches = {}\n",
    "\n",
    "# Iterate through sentences and check if they match any rule\n",
    "for sentence in text_sentences:\n",
    "    sentence_tokens = word_tokenize(sentence)\n",
    "    filtered_sentence_tokens = [word for word in sentence_tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    for rule_name, rule_text in Anomaly_rule_list.items():\n",
    "        rule_tokens = word_tokenize(rule_text)\n",
    "        filtered_rule_tokens = [word for word in rule_tokens if word.lower() not in stop_words]\n",
    "        \n",
    "        # Calculate Jaccard similarity\n",
    "        intersection = len(set(filtered_sentence_tokens).intersection(filtered_rule_tokens))\n",
    "        union = len(set(filtered_sentence_tokens).union(filtered_rule_tokens))\n",
    "        jaccard_similarity = intersection / union\n",
    "        #print(\"jaccard_similarity\", jaccard_similarity)\n",
    "        # If the similarity is above a threshold (e.g., 0.5), consider it a match\n",
    "        if jaccard_similarity > 0.06:\n",
    "            if jaccard_similarity >= 0.23:\n",
    "                if rule_name in matched_sentences:\n",
    "                    matched_sentences[rule_name].append(sentence)\n",
    "                else:\n",
    "                    matched_sentences[rule_name] = [sentence]\n",
    "            else:\n",
    "                if rule_name in partial_matches:\n",
    "                    partial_matches[rule_name].append(sentence)\n",
    "                else:\n",
    "                    partial_matches[rule_name] = [sentence]\n",
    "\n",
    "# Print exact rule matches\n",
    "print(\"Exact Rule Matches:\")\n",
    "for rule_name, matched_sentence_list in matched_sentences.items():\n",
    "    print(f\"Rule '{rule_name}' Matches:\")\n",
    "    for sentence in matched_sentence_list:\n",
    "        print(f\"- {sentence}\")\n",
    "    print()\n",
    "\n",
    "# Print partial rule matches\n",
    "print(\"Partial Rule Matches:\")\n",
    "for rule_name, partial_match_sentence_list in partial_matches.items():\n",
    "    print(f\"Rule '{rule_name}' Partial Matches:\")\n",
    "    for sentence in partial_match_sentence_list:\n",
    "        print(f\"- {sentence}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a043c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "803b0e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Rule Matches:\n",
      "Rule 'Rule 1' Matches:\n",
      "- Education is 'Low' , 'Medium' and Gender is male.\n",
      "\n",
      "Partial Rule Matches:\n",
      "Rule 'Rule 2' Partial Matches:\n",
      "- Education is 'Low' , 'Medium' and Gender is male.\n",
      "- Education is High , Gender is Male age 0\n",
      "\n",
      "Rule 'Rule 1' Partial Matches:\n",
      "- Education is High , Gender is Male age 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "# Define the input text\n",
    "text = \"\"\"Education is 'Low' , 'Medium' and Gender is male. Appendix...\n",
    "Age is 1 2 4 6 89. XYZ ltd. Among the largest university presses in the world,\n",
    "The MIT Press publishes over 200 new books each year along with 30 journals\n",
    "in the arts and humanities, economics, international affairs, history, political\n",
    "science, science and technology along with other disciplines. We were among\n",
    "the first university presses to offer titles electronically and we continue to\n",
    "adopt technologies that allow us to better support the scholarly mission and\n",
    "disseminate our content widely. The Press's enthusiasm for innovation is\n",
    "reflected in our continuing exploration of this frontier. Since the late 1960s,\n",
    "we have experimented with generation after generation of electronic publishing tools.\n",
    "Through our commitment to new products—whether digital journals or entirely\n",
    "new forms of communication—we have continued to look for the most efficient\n",
    "and effective means to serve our readership. Our readers have come to expect\n",
    "excellence from our products, and they can count on us to maintain a commitment\n",
    "to producing rigorous and innovative information products in whatever forms\n",
    "the future of publishing may bring. Education is High , Gender is Male age 0\"\"\"\n",
    "\n",
    "# Define the list of anomaly rules\n",
    "Anomaly_rule_list = {\n",
    "    \"Rule 1\": \"(Education == 'High') AND (Gender == 'Male')\",\n",
    "    \"Rule 2\": \"(Education == 'High') AND (Age > 0) AND (Income > 66500.0)\",\n",
    "}\n",
    "\n",
    "# Tokenize the input text and remove stopwords\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"stopwords\")\n",
    "\n",
    "# Define a list of common stopwords to remove\n",
    "stop_words = [\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
    "    \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\",\n",
    "    \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\",\n",
    "    \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\",\n",
    "    \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\",\n",
    "    \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\",\n",
    "    \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\",\n",
    "    \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\",\n",
    "    \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"\n",
    "]\n",
    "\n",
    "text_sentences = sent_tokenize(text)\n",
    "#stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words = stop_words\n",
    "\n",
    "# Create dictionaries to store the matched sentences and partial matches for each rule\n",
    "matched_sentences = {}\n",
    "partial_matches = {}\n",
    "\n",
    "# Iterate through sentences and check if they match any rule\n",
    "for sentence in text_sentences:\n",
    "    sentence_tokens = word_tokenize(sentence)\n",
    "    filtered_sentence_tokens = [word for word in sentence_tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    for rule_name, rule_text in Anomaly_rule_list.items():\n",
    "        rule_tokens = word_tokenize(rule_text)\n",
    "        filtered_rule_tokens = [word for word in rule_tokens if word.lower() not in stop_words]\n",
    "        \n",
    "        # Calculate Jaccard similarity\n",
    "        intersection = len(set(filtered_sentence_tokens).intersection(filtered_rule_tokens))\n",
    "        union = len(set(filtered_sentence_tokens).union(filtered_rule_tokens))\n",
    "        jaccard_similarity = intersection / union\n",
    "        #print(\"jaccard_similarity\", jaccard_similarity)\n",
    "        # If the similarity is above a threshold (e.g., 0.5), consider it a match\n",
    "        if jaccard_similarity > 0.06:\n",
    "            if jaccard_similarity >= 0.23:\n",
    "                if rule_name in matched_sentences:\n",
    "                    matched_sentences[rule_name].append(sentence)\n",
    "                else:\n",
    "                    matched_sentences[rule_name] = [sentence]\n",
    "            else:\n",
    "                if rule_name in partial_matches:\n",
    "                    partial_matches[rule_name].append(sentence)\n",
    "                else:\n",
    "                    partial_matches[rule_name] = [sentence]\n",
    "\n",
    "# Print exact rule matches\n",
    "print(\"Exact Rule Matches:\")\n",
    "for rule_name, matched_sentence_list in matched_sentences.items():\n",
    "    print(f\"Rule '{rule_name}' Matches:\")\n",
    "    for sentence in matched_sentence_list:\n",
    "        print(f\"- {sentence}\")\n",
    "    print()\n",
    "\n",
    "# Print partial rule matches\n",
    "print(\"Partial Rule Matches:\")\n",
    "for rule_name, partial_match_sentence_list in partial_matches.items():\n",
    "    print(f\"Rule '{rule_name}' Partial Matches:\")\n",
    "    for sentence in partial_match_sentence_list:\n",
    "        print(f\"- {sentence}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb117d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85570fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Rule Matches:\n",
      "Rule 'Rule 1' Matches:\n",
      "- Education is 'Low' , 'Medium' and Gender is male\n",
      "\n",
      "Partial Rule Matches:\n",
      "Rule 'Rule 2' Partial Matches:\n",
      "- Education is 'Low' , 'Medium' and Gender is male\n",
      "- Education is High , Gender is Male\n",
      "\n",
      "Rule 'Rule 1' Partial Matches:\n",
      "- Education is High , Gender is Male\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Define the input text\n",
    "text = \"\"\"Education is 'Low' , 'Medium' and Gender is male. Appendix...\n",
    "Age is 1 2 4 6 89. XYZ ltd. Among the largest university presses in the world,\n",
    "The MIT Press publishes over 200 new books each year along with 30 journals\n",
    "in the arts and humanities, economics, international affairs, history, political\n",
    "science, science and technology along with other disciplines. We were among\n",
    "the first university presses to offer titles electronically and we continue to\n",
    "adopt technologies that allow us to better support the scholarly mission and\n",
    "disseminate our content widely. The Press's enthusiasm for innovation is\n",
    "reflected in our continuing exploration of this frontier. Since the late 1960s,\n",
    "we have experimented with generation after generation of electronic publishing tools.\n",
    "Through our commitment to new products—whether digital journals or entirely\n",
    "new forms of communication—we have continued to look for the most efficient\n",
    "and effective means to serve our readership. Our readers have come to expect\n",
    "excellence from our products, and they can count on us to maintain a commitment\n",
    "to producing rigorous and innovative information products in whatever forms\n",
    "the future of publishing may bring. Education is High , Gender is Male.\"\"\"\n",
    "\n",
    "# Define the list of anomaly rules\n",
    "Anomaly_rule_list = {\n",
    "    \"Rule 1\": \"(Education == 'High') AND (Gender == 'Male')\",\n",
    "    \"Rule 2\": \"(Education == 'High') AND (Age > 0) AND (Income > 66500.0)\",\n",
    "}\n",
    "\n",
    "# Tokenize the input text\n",
    "text_sentences = text.split('.')  # Split the text into sentences based on periods\n",
    "\n",
    "# Define a list of common stopwords to remove\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Create dictionaries to store the matched sentences and partial matches for each rule\n",
    "matched_sentences = {}\n",
    "partial_matches = {}\n",
    "\n",
    "# Define a function to preprocess and extract main words from a rule\n",
    "def extract_main_words(rule_text):\n",
    "    # Remove extra spaces and 'AND' from the rule text\n",
    "    rule_text = re.sub(r'\\s+', ' ', rule_text)  # Remove extra spaces\n",
    "    rule_text = re.sub(r'\\bAND\\b', '', rule_text)  # Remove 'AND' (word boundary)\n",
    "\n",
    "    # Tokenize the rule text and remove stopwords\n",
    "    rule_tokens = word_tokenize(rule_text)\n",
    "    filtered_rule_tokens = [word.lower() for word in rule_tokens if word.lower() not in stop_words]\n",
    "\n",
    "    return set(filtered_rule_tokens)\n",
    "\n",
    "# Iterate through sentences and check if they match any rule\n",
    "for sentence in text_sentences:\n",
    "    sentence_tokens = word_tokenize(sentence.lower())\n",
    "    \n",
    "    for rule_name, rule_text in Anomaly_rule_list.items():\n",
    "        rule_main_words = extract_main_words(rule_text.lower())\n",
    "        #print(\"rule_main_words\", rule_main_words)\n",
    "        # Calculate Jaccard similarity\n",
    "        intersection = len(set(sentence_tokens).intersection(rule_main_words))\n",
    "        union = len(set(sentence_tokens).union(rule_main_words))\n",
    "        jaccard_similarity = intersection / union\n",
    "        \n",
    "        # If the similarity is above a threshold (e.g., 0.5), consider it a match\n",
    "        if jaccard_similarity > 0.06:\n",
    "            if jaccard_similarity > 0.2:\n",
    "                if rule_name in matched_sentences:\n",
    "                    matched_sentences[rule_name].append(sentence)\n",
    "                else:\n",
    "                    matched_sentences[rule_name] = [sentence]\n",
    "            else:\n",
    "                if rule_name in partial_matches:\n",
    "                    partial_matches[rule_name].append(sentence)\n",
    "                else:\n",
    "                    partial_matches[rule_name] = [sentence]\n",
    "\n",
    "# Print exact rule matches\n",
    "print(\"Exact Rule Matches:\")\n",
    "for rule_name, matched_sentence_list in matched_sentences.items():\n",
    "    print(f\"Rule '{rule_name}' Matches:\")\n",
    "    for sentence in matched_sentence_list:\n",
    "        print(f\"- {sentence.strip()}\")\n",
    "    print()\n",
    "\n",
    "# Print partial rule matches\n",
    "print(\"Partial Rule Matches:\")\n",
    "for rule_name, partial_match_sentence_list in partial_matches.items():\n",
    "    print(f\"Rule '{rule_name}' Partial Matches:\")\n",
    "    for sentence in partial_match_sentence_list:\n",
    "        print(f\"- {sentence.strip()}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43f012c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9af8d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Rule Matches:\n",
      "Rule 'Rule 1' Matches:\n",
      "- Education is 'Low' , 'Medium' and Gender is male\n",
      "\n",
      "Partial Rule Matches:\n",
      "Rule 'Rule 2' Partial Matches:\n",
      "- Education is 'Low' , 'Medium' and Gender is male\n",
      "- Education is High , Gender is Male\n",
      "\n",
      "Rule 'Rule 1' Partial Matches:\n",
      "- Education is High , Gender is Male\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Define the input text\n",
    "text = \"\"\"Education is 'Low' , 'Medium' and Gender is male. Appendix...\n",
    "Age is 1 2 4 6 89. XYZ ltd. Among the largest university presses in the world,\n",
    "The MIT Press publishes over 200 new books each year along with 30 journals\n",
    "in the arts and humanities, economics, international affairs, history, political\n",
    "science, science and technology along with other disciplines. We were among\n",
    "the first university presses to offer titles electronically and we continue to\n",
    "adopt technologies that allow us to better support the scholarly mission and\n",
    "disseminate our content widely. The Press's enthusiasm for innovation is\n",
    "reflected in our continuing exploration of this frontier. Since the late 1960s,\n",
    "we have experimented with generation after generation of electronic publishing tools.\n",
    "Through our commitment to new products—whether digital journals or entirely\n",
    "new forms of communication—we have continued to look for the most efficient\n",
    "and effective means to serve our readership. Our readers have come to expect\n",
    "excellence from our products, and they can count on us to maintain a commitment\n",
    "to producing rigorous and innovative information products in whatever forms\n",
    "the future of publishing may bring. Education is High , Gender is Male.\"\"\"\n",
    "\n",
    "# Define the list of anomaly rules\n",
    "Anomaly_rule_list = {\n",
    "    \"Rule 1\": \"(Education == 'High') AND (Gender == 'Male')\",\n",
    "    \"Rule 2\": \"(Education == 'High') AND (Age > 0) AND (Income > 66500.0)\",\n",
    "}\n",
    "\n",
    "# Tokenize the input text\n",
    "text_sentences = text.split('.')  # Split the text into sentences based on periods\n",
    "\n",
    "# Define a list of common stopwords to remove\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Define a function to preprocess and extract main words and operators from a rule\n",
    "def extract_main_words(rule_text):\n",
    "    # Remove extra spaces, 'AND', and punctuation\n",
    "    rule_text = re.sub(r'\\s+', ' ', rule_text)  # Remove extra spaces\n",
    "    rule_text = re.sub(r'\\bAND\\b', '', rule_text)  # Remove 'AND' (word boundary)\n",
    "    rule_text = re.sub(r'[^\\w\\s]', '', rule_text)  # Remove punctuation\n",
    "    \n",
    "    # Tokenize the rule text and remove stopwords\n",
    "    rule_tokens = word_tokenize(rule_text)\n",
    "    filtered_rule_tokens = [word.lower() for word in rule_tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Extract main words and operators\n",
    "    main_words = []\n",
    "    for i, token in enumerate(filtered_rule_tokens):\n",
    "        if token in ['is', '=', '==']:  # Consider 'is', '=', or '==' as operators\n",
    "            if i > 0:\n",
    "                main_words.append(filtered_rule_tokens[i - 1])\n",
    "                main_words.append(token)\n",
    "            if i < len(filtered_rule_tokens) - 1:\n",
    "                main_words.append(filtered_rule_tokens[i + 1])\n",
    "    \n",
    "    return set(main_words)\n",
    "\n",
    "# Iterate through sentences and check if they match any rule\n",
    "for sentence in text_sentences:\n",
    "    sentence_tokens = word_tokenize(sentence.lower())\n",
    "    \n",
    "    for rule_name, rule_text in Anomaly_rule_list.items():\n",
    "        rule_main_words = extract_main_words(rule_text.lower())\n",
    "        \n",
    "        # Calculate Jaccard similarity\n",
    "        intersection = len(set(sentence_tokens).intersection(rule_main_words))\n",
    "        union = len(set(sentence_tokens).union(rule_main_words))\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        jaccard_similarity = intersection / union if union != 0 else 0\n",
    "        \n",
    "        # If the similarity is above a threshold (e.g., 0.5), consider it a match\n",
    "        if jaccard_similarity > 0.5:\n",
    "            if jaccard_similarity > 0.3:\n",
    "                if rule_name in matched_sentences:\n",
    "                    matched_sentences[rule_name].append(sentence)\n",
    "                else:\n",
    "                    matched_sentences[rule_name] = [sentence]\n",
    "            else:\n",
    "                if rule_name in partial_matches:\n",
    "                    partial_matches[rule_name].append(sentence)\n",
    "                else:\n",
    "                    partial_matches[rule_name] = [sentence]\n",
    "\n",
    "# Print exact rule matches\n",
    "print(\"Exact Rule Matches:\")\n",
    "for rule_name, matched_sentence_list in matched_sentences.items():\n",
    "    print(f\"Rule '{rule_name}' Matches:\")\n",
    "    for sentence in matched_sentence_list:\n",
    "        print(f\"- {sentence.strip()}\")\n",
    "    print()\n",
    "\n",
    "# Print partial rule matches\n",
    "print(\"Partial Rule Matches:\")\n",
    "for rule_name, partial_match_sentence_list in partial_matches.items():\n",
    "    print(f\"Rule '{rule_name}' Partial Matches:\")\n",
    "    for sentence in partial_match_sentence_list:\n",
    "        print(f\"- {sentence.strip()}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d75391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8053790d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Rule Matches:\n",
      "Rule 'Rule 1' Matches:\n",
      "- Education is 'Low' , 'Medium' and Gender is male\n",
      "- Education is 'Low' , 'Medium' and Gender is male\n",
      "- Appendix\n",
      "- \n",
      "- \n",
      "- Age is 1 2 4 6 89\n",
      "- XYZ ltd\n",
      "- Among the largest university presses in the world,\n",
      "The MIT Press publishes over 200 new books each year along with 30 journals\n",
      "in the arts and humanities, economics, international affairs, history, political\n",
      "science, science and technology along with other disciplines\n",
      "- We were among\n",
      "the first university presses to offer titles electronically and we continue to\n",
      "adopt technologies that allow us to better support the scholarly mission and\n",
      "disseminate our content widely\n",
      "- The Press's enthusiasm for innovation is\n",
      "reflected in our continuing exploration of this frontier\n",
      "- Since the late 1960s,\n",
      "we have experimented with generation after generation of electronic publishing tools\n",
      "- Through our commitment to new products—whether digital journals or entirely\n",
      "new forms of communication—we have continued to look for the most efficient\n",
      "and effective means to serve our readership\n",
      "- Our readers have come to expect\n",
      "excellence from our products, and they can count on us to maintain a commitment\n",
      "to producing rigorous and innovative information products in whatever forms\n",
      "the future of publishing may bring\n",
      "- Education is High , Gender is Male\n",
      "- \n",
      "\n",
      "Rule 'Rule 2' Matches:\n",
      "- Education is 'Low' , 'Medium' and Gender is male\n",
      "- Appendix\n",
      "- \n",
      "- \n",
      "- Age is 1 2 4 6 89\n",
      "- XYZ ltd\n",
      "- Among the largest university presses in the world,\n",
      "The MIT Press publishes over 200 new books each year along with 30 journals\n",
      "in the arts and humanities, economics, international affairs, history, political\n",
      "science, science and technology along with other disciplines\n",
      "- We were among\n",
      "the first university presses to offer titles electronically and we continue to\n",
      "adopt technologies that allow us to better support the scholarly mission and\n",
      "disseminate our content widely\n",
      "- The Press's enthusiasm for innovation is\n",
      "reflected in our continuing exploration of this frontier\n",
      "- Since the late 1960s,\n",
      "we have experimented with generation after generation of electronic publishing tools\n",
      "- Through our commitment to new products—whether digital journals or entirely\n",
      "new forms of communication—we have continued to look for the most efficient\n",
      "and effective means to serve our readership\n",
      "- Our readers have come to expect\n",
      "excellence from our products, and they can count on us to maintain a commitment\n",
      "to producing rigorous and innovative information products in whatever forms\n",
      "the future of publishing may bring\n",
      "- Education is High , Gender is Male\n",
      "- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Define the input text\n",
    "text = \"\"\"Education is 'Low' , 'Medium' and Gender is male. Appendix...\n",
    "Age is 1 2 4 6 89. XYZ ltd. Among the largest university presses in the world,\n",
    "The MIT Press publishes over 200 new books each year along with 30 journals\n",
    "in the arts and humanities, economics, international affairs, history, political\n",
    "science, science and technology along with other disciplines. We were among\n",
    "the first university presses to offer titles electronically and we continue to\n",
    "adopt technologies that allow us to better support the scholarly mission and\n",
    "disseminate our content widely. The Press's enthusiasm for innovation is\n",
    "reflected in our continuing exploration of this frontier. Since the late 1960s,\n",
    "we have experimented with generation after generation of electronic publishing tools.\n",
    "Through our commitment to new products—whether digital journals or entirely\n",
    "new forms of communication—we have continued to look for the most efficient\n",
    "and effective means to serve our readership. Our readers have come to expect\n",
    "excellence from our products, and they can count on us to maintain a commitment\n",
    "to producing rigorous and innovative information products in whatever forms\n",
    "the future of publishing may bring. Education is High , Gender is Male.\"\"\"\n",
    "\n",
    "# Define the list of anomaly rules\n",
    "Anomaly_rule_list = {\n",
    "    \"Rule 1\": \"(Education == 'High') AND (Gender == 'Male')\",\n",
    "    \"Rule 2\": \"(Education == 'High') AND (Age > 0) AND (Income > 66500.0)\",\n",
    "}\n",
    "\n",
    "# Tokenize the input text\n",
    "text_sentences = text.split('.')  # Split the text into sentences based on periods\n",
    "\n",
    "# Define a list of common stopwords to remove\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Define a function to preprocess and extract main words and operators from a rule\n",
    "def extract_main_words(rule_text):\n",
    "    # Remove extra spaces, 'AND', and punctuation\n",
    "    rule_text = re.sub(r'\\s+', ' ', rule_text)  # Remove extra spaces\n",
    "    rule_text = re.sub(r'\\bAND\\b', '', rule_text)  # Remove 'AND' (word boundary)\n",
    "    rule_text = re.sub(r'[^\\w\\s]', '', rule_text)  # Remove punctuation\n",
    "    \n",
    "    # Tokenize the rule text and remove stopwords\n",
    "    rule_tokens = word_tokenize(rule_text)\n",
    "    filtered_rule_tokens = [word.lower() for word in rule_tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Extract main words and operators\n",
    "    main_words = []\n",
    "    for i, token in enumerate(filtered_rule_tokens):\n",
    "        if token in ['is', '=', '==']:  # Consider 'is', '=', or '==' as operators\n",
    "            if i > 0:\n",
    "                main_words.append(filtered_rule_tokens[i - 1])\n",
    "                main_words.append(token)\n",
    "            if i < len(filtered_rule_tokens) - 1:\n",
    "                main_words.append(filtered_rule_tokens[i + 1])\n",
    "    \n",
    "    return set(main_words)\n",
    "\n",
    "# Define a function to check if a rule matches a sentence\n",
    "def rule_matches_sentence(rule_main_words, sentence_tokens):\n",
    "    for i in range(len(sentence_tokens) - len(rule_main_words) + 1):\n",
    "        window = sentence_tokens[i:i + len(rule_main_words)]\n",
    "        if all(word in window for word in rule_main_words):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Iterate through sentences and check if they match any rule\n",
    "for sentence in text_sentences:\n",
    "    sentence_tokens = word_tokenize(sentence.lower())\n",
    "    \n",
    "    for rule_name, rule_text in Anomaly_rule_list.items():\n",
    "        rule_main_words = extract_main_words(rule_text.lower())\n",
    "        \n",
    "        # Check if the rule matches the sentence\n",
    "        if rule_matches_sentence(rule_main_words, sentence_tokens):\n",
    "            if rule_name in matched_sentences:\n",
    "                matched_sentences[rule_name].append(sentence)\n",
    "            else:\n",
    "                matched_sentences[rule_name] = [sentence]\n",
    "\n",
    "# Print exact rule matches\n",
    "print(\"Exact Rule Matches:\")\n",
    "for rule_name, matched_sentence_list in matched_sentences.items():\n",
    "    print(f\"Rule '{rule_name}' Matches:\")\n",
    "    for sentence in matched_sentence_list:\n",
    "        print(f\"- {sentence.strip()}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f528258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b6ca1489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Education is 'Low' , 'Medium' and Gender is male.\n",
      "Rule 'Rule 1' Context: (Education == 'High') AND (Gender == 'Male')\n",
      "\n",
      "Sentence: Education is 'Low' , 'Medium' and Gender is male.\n",
      "Rule 'Rule 2' Context: (Education == 'High') AND (Age > 0) AND (Income > 66500.0)\n",
      "\n",
      "Sentence: Appendix...\n",
      "Age is 1 2 4 6 89.\n",
      "Rule 'Rule 1' Context: (Education == 'High') AND (Gender == 'Male')\n",
      "\n",
      "Sentence: Appendix...\n",
      "Age is 1 2 4 6 89.\n",
      "Rule 'Rule 2' Context: (Education == 'High') AND (Age > 0) AND (Income > 66500.0)\n",
      "\n",
      "Sentence: XYZ ltd.\n",
      "Rule 'Rule 1' Context: (Education == 'High') AND (Gender == 'Male')\n",
      "\n",
      "Sentence: XYZ ltd.\n",
      "Rule 'Rule 2' Context: (Education == 'High') AND (Age > 0) AND (Income > 66500.0)\n",
      "\n",
      "Sentence: Among the largest university presses in the world,\n",
      "The MIT Press publishes over 200 new books each year along with 30 journals\n",
      "in the arts and humanities, economics, international affairs, history, political\n",
      "science, science and technology along with other disciplines.\n",
      "Rule 'Rule 1' Context: (Education == 'High') AND (Gender == 'Male')\n",
      "\n",
      "Sentence: Among the largest university presses in the world,\n",
      "The MIT Press publishes over 200 new books each year along with 30 journals\n",
      "in the arts and humanities, economics, international affairs, history, political\n",
      "science, science and technology along with other disciplines.\n",
      "Rule 'Rule 2' Context: (Education == 'High') AND (Age > 0) AND (Income > 66500.0)\n",
      "\n",
      "Sentence: We were among\n",
      "the first university presses to offer titles electronically and we continue to\n",
      "adopt technologies that allow us to better support the scholarly mission and\n",
      "disseminate our content widely.\n",
      "Rule 'Rule 1' Context: (Education == 'High') AND (Gender == 'Male')\n",
      "\n",
      "Sentence: We were among\n",
      "the first university presses to offer titles electronically and we continue to\n",
      "adopt technologies that allow us to better support the scholarly mission and\n",
      "disseminate our content widely.\n",
      "Rule 'Rule 2' Context: (Education == 'High') AND (Age > 0) AND (Income > 66500.0)\n",
      "\n",
      "Sentence: The Press's enthusiasm for innovation is\n",
      "reflected in our continuing exploration of this frontier.\n",
      "Rule 'Rule 1' Context: (Education == 'High') AND (Gender == 'Male')\n",
      "\n",
      "Sentence: The Press's enthusiasm for innovation is\n",
      "reflected in our continuing exploration of this frontier.\n",
      "Rule 'Rule 2' Context: (Education == 'High') AND (Age > 0) AND (Income > 66500.0)\n",
      "\n",
      "Sentence: Since the late 1960s,\n",
      "we have experimented with generation after generation of electronic publishing tools.\n",
      "Rule 'Rule 1' Context: (Education == 'High') AND (Gender == 'Male')\n",
      "\n",
      "Sentence: Since the late 1960s,\n",
      "we have experimented with generation after generation of electronic publishing tools.\n",
      "Rule 'Rule 2' Context: (Education == 'High') AND (Age > 0) AND (Income > 66500.0)\n",
      "\n",
      "Sentence: Through our commitment to new products—whether digital journals or entirely\n",
      "new forms of communication—we have continued to look for the most efficient\n",
      "and effective means to serve our readership.\n",
      "Rule 'Rule 1' Context: (Education == 'High') AND (Gender == 'Male')\n",
      "\n",
      "Sentence: Through our commitment to new products—whether digital journals or entirely\n",
      "new forms of communication—we have continued to look for the most efficient\n",
      "and effective means to serve our readership.\n",
      "Rule 'Rule 2' Context: (Education == 'High') AND (Age > 0) AND (Income > 66500.0)\n",
      "\n",
      "Sentence: Our readers have come to expect\n",
      "excellence from our products, and they can count on us to maintain a commitment\n",
      "to producing rigorous and innovative information products in whatever forms\n",
      "the future of publishing may bring.\n",
      "Rule 'Rule 1' Context: (Education == 'High') AND (Gender == 'Male')\n",
      "\n",
      "Sentence: Our readers have come to expect\n",
      "excellence from our products, and they can count on us to maintain a commitment\n",
      "to producing rigorous and innovative information products in whatever forms\n",
      "the future of publishing may bring.\n",
      "Rule 'Rule 2' Context: (Education == 'High') AND (Age > 0) AND (Income > 66500.0)\n",
      "\n",
      "Sentence: Education is High , Gender is Male.\n",
      "Rule 'Rule 1' Context: (Education == 'High') AND (Gender == 'Male')\n",
      "\n",
      "Sentence: Education is High , Gender is Male.\n",
      "Rule 'Rule 2' Context: (Education == 'High') AND (Age > 0) AND (Income > 66500.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Define the input text\n",
    "text = \"\"\"Education is 'Low' , 'Medium' and Gender is male. Appendix...\n",
    "Age is 1 2 4 6 89. XYZ ltd. Among the largest university presses in the world,\n",
    "The MIT Press publishes over 200 new books each year along with 30 journals\n",
    "in the arts and humanities, economics, international affairs, history, political\n",
    "science, science and technology along with other disciplines. We were among\n",
    "the first university presses to offer titles electronically and we continue to\n",
    "adopt technologies that allow us to better support the scholarly mission and\n",
    "disseminate our content widely. The Press's enthusiasm for innovation is\n",
    "reflected in our continuing exploration of this frontier. Since the late 1960s,\n",
    "we have experimented with generation after generation of electronic publishing tools.\n",
    "Through our commitment to new products—whether digital journals or entirely\n",
    "new forms of communication—we have continued to look for the most efficient\n",
    "and effective means to serve our readership. Our readers have come to expect\n",
    "excellence from our products, and they can count on us to maintain a commitment\n",
    "to producing rigorous and innovative information products in whatever forms\n",
    "the future of publishing may bring. Education is High , Gender is Male.\"\"\"\n",
    "\n",
    "# Define the list of anomaly rules\n",
    "Anomaly_rule_list = {\n",
    "    \"Rule 1\": \"(Education == 'High') AND (Gender == 'Male')\",\n",
    "    \"Rule 2\": \"(Education == 'High') AND (Age > 0) AND (Income > 66500.0)\",\n",
    "}\n",
    "\n",
    "# Tokenize the input text into sentences\n",
    "text_sentences = sent_tokenize(text)\n",
    "\n",
    "# Define a list of common stopwords to remove\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Define a function to preprocess and extract main words and operators from a rule\n",
    "def extract_main_words(rule_text):\n",
    "    # Remove extra spaces, 'AND', and punctuation\n",
    "    rule_text = re.sub(r'\\s+', ' ', rule_text)  # Remove extra spaces\n",
    "    rule_text = re.sub(r'\\bAND\\b', '', rule_text)  # Remove 'AND' (word boundary)\n",
    "    rule_text = re.sub(r'[^\\w\\s]', '', rule_text)  # Remove punctuation\n",
    "    \n",
    "    # Tokenize the rule text and remove stopwords\n",
    "    rule_tokens = word_tokenize(rule_text)\n",
    "    filtered_rule_tokens = [word.lower() for word in rule_tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Extract main words and operators\n",
    "    main_words = []\n",
    "    for i, token in enumerate(filtered_rule_tokens):\n",
    "        if token in ['is', '=', '==']:  # Consider 'is', '=', or '==' as operators\n",
    "            if i > 0:\n",
    "                main_words.append(filtered_rule_tokens[i - 1])\n",
    "                main_words.append(token)\n",
    "            if i < len(filtered_rule_tokens) - 1:\n",
    "                main_words.append(filtered_rule_tokens[i + 1])\n",
    "    \n",
    "    return set(main_words)\n",
    "\n",
    "# Define a function to check if a rule matches a sentence\n",
    "def rule_matches_sentence(rule_main_words, sentence_tokens):\n",
    "    for i in range(len(sentence_tokens) - len(rule_main_words) + 1):\n",
    "        window = sentence_tokens[i:i + len(rule_main_words)]\n",
    "        if all(word in window for word in rule_main_words):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Iterate through sentences and check if they match any rule\n",
    "for sentence in text_sentences:\n",
    "    sentence_tokens = word_tokenize(sentence.lower())\n",
    "    \n",
    "    for rule_name, rule_text in Anomaly_rule_list.items():\n",
    "        rule_main_words = extract_main_words(rule_text.lower())\n",
    "        \n",
    "        # Check if the rule matches the sentence\n",
    "        if rule_matches_sentence(rule_main_words, sentence_tokens):\n",
    "            print(f\"Sentence: {sentence.strip()}\")  # Print the matching sentence\n",
    "            print(f\"Rule '{rule_name}' Context: {rule_text}\")  # Print the matching rule context\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b6bd4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ec12e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a761d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_main_words set()\n",
      "rule_main_words set()\n",
      "rule_main_words set()\n",
      "rule_main_words set()\n",
      "rule_main_words set()\n",
      "rule_main_words set()\n",
      "rule_main_words set()\n",
      "rule_main_words set()\n",
      "rule_main_words set()\n",
      "rule_main_words set()\n",
      "rule_main_words set()\n",
      "rule_main_words set()\n",
      "rule_main_words set()\n",
      "rule_main_words set()\n",
      "rule_main_words set()\n",
      "rule_main_words set()\n",
      "rule_main_words set()\n",
      "rule_main_words set()\n",
      "rule_main_words set()\n",
      "rule_main_words set()\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Define the input text\n",
    "text = \"\"\"Education is 'Low' , 'Medium' and Gender is male. Appendix...\n",
    "Age is 1 2 4 6 89. XYZ ltd. Among the largest university presses in the world,\n",
    "The MIT Press publishes over 200 new books each year along with 30 journals\n",
    "in the arts and humanities, economics, international affairs, history, political\n",
    "science, science and technology along with other disciplines. We were among\n",
    "the first university presses to offer titles electronically and we continue to\n",
    "adopt technologies that allow us to better support the scholarly mission and\n",
    "disseminate our content widely. The Press's enthusiasm for innovation is\n",
    "reflected in our continuing exploration of this frontier. Since the late 1960s,\n",
    "we have experimented with generation after generation of electronic publishing tools.\n",
    "Through our commitment to new products—whether digital journals or entirely\n",
    "new forms of communication—we have continued to look for the most efficient\n",
    "and effective means to serve our readership. Our readers have come to expect\n",
    "excellence from our products, and they can count on us to maintain a commitment\n",
    "to producing rigorous and innovative information products in whatever forms\n",
    "the future of publishing may bring. Education is High , Gender is Male.\"\"\"\n",
    "\n",
    "# Define the list of anomaly rules\n",
    "Anomaly_rule_list = {\n",
    "    \"Rule 1\": \"(Education == 'High') AND (Gender == 'Male')\",\n",
    "    \"Rule 2\": \"(Education == 'High') AND (Age > 0) AND (Income > 66500.0)\",\n",
    "}\n",
    "\n",
    "# Tokenize the input text into sentences\n",
    "text_sentences = sent_tokenize(text)\n",
    "\n",
    "# Define a list of common stopwords to remove\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Define a function to preprocess and extract main words and operators from a rule\n",
    "def extract_main_words(rule_text):\n",
    "    # Remove extra spaces, 'AND', and punctuation\n",
    "    rule_text = re.sub(r'\\s+', ' ', rule_text)  # Remove extra spaces\n",
    "    rule_text = re.sub(r'\\bAND\\b', '', rule_text)  # Remove 'AND' (word boundary)\n",
    "    rule_text = re.sub(r'[^\\w\\s]', '', rule_text)  # Remove punctuation\n",
    "    \n",
    "    # Tokenize the rule text and remove stopwords\n",
    "    rule_tokens = word_tokenize(rule_text)\n",
    "    filtered_rule_tokens = [word.lower() for word in rule_tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Extract main words and operators\n",
    "    main_words = []\n",
    "    for i, token in enumerate(filtered_rule_tokens):\n",
    "        if token in ['is', '=', '==']:  # Consider 'is', '=', or '==' as operators\n",
    "            if i > 0:\n",
    "                main_words.append(filtered_rule_tokens[i - 1])\n",
    "                main_words.append(token)\n",
    "            if i < len(filtered_rule_tokens) - 1:\n",
    "                main_words.append(filtered_rule_tokens[i + 1])\n",
    "    \n",
    "    return set(main_words)\n",
    "\n",
    "# Define a function to calculate Jaccard similarity\n",
    "def calculate_jaccard_similarity(rule_main_words, sentence_tokens):\n",
    "    intersection = len(set(sentence_tokens).intersection(rule_main_words))\n",
    "    union = len(set(sentence_tokens).union(rule_main_words))\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    jaccard_similarity = intersection / union if union != 0 else 0\n",
    "    \n",
    "    return jaccard_similarity\n",
    "\n",
    "# Define a function to check if a rule matches a sentence\n",
    "def rule_matches_sentence(rule_main_words, sentence_tokens, min_jaccard_similarity=0.5):\n",
    "    for i in range(len(sentence_tokens) - len(rule_main_words) + 1):\n",
    "        window = sentence_tokens[i:i + len(rule_main_words)]\n",
    "        if all(word in window for word in rule_main_words):\n",
    "            jaccard_similarity = calculate_jaccard_similarity(rule_main_words, sentence_tokens)\n",
    "            if jaccard_similarity >= min_jaccard_similarity:\n",
    "                return True, jaccard_similarity\n",
    "    return False, 0\n",
    "\n",
    "# Define the minimum Jaccard similarity threshold\n",
    "min_jaccard_similarity_threshold = 0.01 # Adjust as needed\n",
    "\n",
    "# Iterate through sentences and check if they match any rule\n",
    "for sentence in text_sentences:\n",
    "    sentence_tokens = word_tokenize(sentence.lower())\n",
    "    \n",
    "    for rule_name, rule_text in Anomaly_rule_list.items():\n",
    "        rule_main_words = extract_main_words(rule_text.lower())\n",
    "        print(\"rule_main_words\", rule_main_words)\n",
    "        # Check if the rule matches the sentence with the specified Jaccard similarity threshold\n",
    "        matched, jaccard_similarity = rule_matches_sentence(rule_main_words, sentence_tokens, min_jaccard_similarity_threshold)\n",
    "        if matched:\n",
    "            print(f\"Sentence: {sentence.strip()}\")  # Print the matching sentence\n",
    "            print(f\"Rule '{rule_name}' Context: {rule_text}\")  # Print the matching rule context\n",
    "            print(f\"Jaccard Similarity: {jaccard_similarity:.2f}\")  # Print Jaccard similarity score\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5291688d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
