{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cad956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset\n",
    "data = [\n",
    "    (1, \"Product A\", \"Category 1\", 100, 50),\n",
    "    (2, \"Product B\", \"Category 1\", 200, 80),\n",
    "    (3, \"Product C\", \"Category 2\", 150, 60),\n",
    "    (4, \"Product D\", \"Category 2\", 300, 90),\n",
    "    (5, \"Product E\", \"Category 3\", 250, 70)\n",
    "]\n",
    "\n",
    "# Creating a PySpark DataFrame\n",
    "df = spark.createDataFrame(data, [\"ID\", \"Product\", \"Category\", \"Price\", \"Quantity\"])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef0290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bar Chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afdec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculating the total sales for each product\n",
    "sales_by_product = df.groupBy(\"Product\").sum(\"Price\").toPandas()\n",
    "\n",
    "# Plotting a bar chart of total sales by product\n",
    "plt.bar(sales_by_product[\"Product\"], sales_by_product[\"sum(Price)\"])\n",
    "plt.xlabel(\"Product\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.title(\"Bar Chart: Total Sales by Product\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3642774",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pie Chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9e7d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculating the total sales for each category\n",
    "sales_by_category = df.groupBy(\"Category\").sum(\"Price\").toPandas()\n",
    "\n",
    "# Plotting a pie chart of total sales by category\n",
    "plt.pie(sales_by_category[\"sum(Price)\"], labels=sales_by_category[\"Category\"], autopct='%1.1f%%')\n",
    "plt.title(\"Pie Chart: Total Sales by Category\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32e9a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Box Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a1775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Creating a box plot of price by category\n",
    "sns.boxplot(data=df.toPandas(), x=\"Category\", y=\"Price\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Box Plot: Price by Category\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f335d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scatter Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc66f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting a scatter plot of price versus quantity\n",
    "price_values = df.select(\"Price\").rdd.flatMap(lambda x: x).collect()\n",
    "quantity_values = df.select(\"Quantity\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "plt.scatter(price_values, quantity_values)\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"Quantity\")\n",
    "plt.title(\"Scatter Plot: Price vs Quantity\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca9d51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Line Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13052ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sorting the DataFrame by ID\n",
    "sorted_df = df.sort(\"ID\")\n",
    "\n",
    "# Line plot of total sales by ID\n",
    "id_values = sorted_df.select(\"ID\").rdd.flatMap(lambda x: x).collect()\n",
    "sales_values = sorted_df.select(\"Price\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "plt.plot(id_values, sales_values)\n",
    "plt.xlabel(\"ID\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.title(\"Line Plot: Total Sales by ID\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aecf6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2093b573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d95c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area chart\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculating the total sales for each product\n",
    "sales_by_product = df.groupBy(\"Product\").sum(\"Price\").toPandas()\n",
    "\n",
    "# Plotting an area chart of total sales by product\n",
    "plt.stackplot(sales_by_product[\"Product\"], sales_by_product[\"sum(Price)\"], labels=sales_by_product[\"Product\"])\n",
    "plt.xlabel(\"Product\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.title(\"Area Chart: Total Sales by Product\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95139b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting a histogram of prices\n",
    "price_values = df.select(\"Price\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "plt.hist(price_values, bins=10)\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram: Price Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198c596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1475b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Calculating the correlation matrix\n",
    "correlation_matrix = df.select(df.columns).toPandas().corr()\n",
    "\n",
    "# Plotting a heatmap of the correlation matrix\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Heatmap: Correlation Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34806aba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30487ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Violin Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a7eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Creating a violin plot of price by category\n",
    "sns.violinplot(data=df.toPandas(), x=\"Category\", y=\"Price\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Violin Plot: Price by Category\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16681f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Radar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b563ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Aggregating the average price and quantity by category\n",
    "avg_price_quantity = df.groupBy(\"Category\").avg(\"Price\", \"Quantity\").toPandas()\n",
    "\n",
    "# Creating data for the radar chart\n",
    "categories = avg_price_quantity[\"Category\"]\n",
    "price_values = avg_price_quantity[\"avg(Price)\"]\n",
    "quantity_values = avg_price_quantity[\"avg(Quantity)\"]\n",
    "\n",
    "# Plotting a radar chart of average price and quantity by category\n",
    "fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
    "angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "ax.plot(angles, price_values.tolist() + price_values.tolist()[:1], label=\"Average Price\")\n",
    "ax.plot(angles, quantity_values.tolist() + quantity_values.tolist()[:1], label=\"Average Quantity\")\n",
    "ax.fill(angles, price_values.tolist() + price_values.tolist()[:1], alpha=0.25)\n",
    "ax.fill(angles, quantity_values.tolist() + quantity_values.tolist()[:1], alpha=0.25)\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories)\n",
    "plt.title(\"Radar Chart: Average Price and Quantity by Category\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14ce72f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d71914",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stacked Bar Chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9bd6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculating the total sales for each category and product\n",
    "sales_by_category_product = df.groupBy(\"Category\", \"Product\").sum(\"Price\").toPandas()\n",
    "\n",
    "# Pivot the data for stacked bar chart\n",
    "pivot_df = sales_by_category_product.pivot(index=\"Category\", columns=\"Product\", values=\"sum(Price)\")\n",
    "\n",
    "# Plotting a stacked bar chart\n",
    "pivot_df.plot(kind=\"bar\", stacked=True)\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.title(\"Stacked Bar Chart: Total Sales by Category and Product\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0ec268",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scatter Matrix Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c83e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Converting the PySpark DataFrame to Pandas DataFrame\n",
    "pandas_df = df.select(\"Price\", \"Quantity\").toPandas()\n",
    "\n",
    "# Plotting a scatter matrix plot\n",
    "sns.pairplot(pandas_df)\n",
    "plt.title(\"Scatter Matrix Plot: Price and Quantity\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c11cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "Word Cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e234e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Concatenating all product names\n",
    "all_products = \" \".join(df.select(\"Product\").rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "# Generating a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(all_products)\n",
    "\n",
    "# Displaying the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud: Product Names\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a5790",
   "metadata": {},
   "outputs": [],
   "source": [
    "Treemap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed9e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import squarify\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculating the total sales for each product\n",
    "sales_by_product = df.groupBy(\"Product\").sum(\"Price\").toPandas()\n",
    "\n",
    "# Sorting the data by total sales\n",
    "sales_by_product.sort_values(\"sum(Price)\", ascending=False, inplace=True)\n",
    "\n",
    "# Creating a treemap\n",
    "squarify.plot(sizes=sales_by_product[\"sum(Price)\"], label=sales_by_product[\"Product\"], alpha=0.8)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Treemap: Total Sales by Product\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271e04ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7e59cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "3D Scatter Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26287181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Extracting the X, Y, and Z coordinates\n",
    "x_values = df.select(\"ID\").rdd.flatMap(lambda x: x).collect()\n",
    "y_values = df.select(\"Price\").rdd.flatMap(lambda x: x).collect()\n",
    "z_values = df.select(\"Quantity\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Creating a 3D scatter plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x_values, y_values, z_values)\n",
    "ax.set_xlabel(\"ID\")\n",
    "ax.set_ylabel(\"Price\")\n",
    "ax.set_zlabel(\"Quantity\")\n",
    "ax.set_title(\"3D Scatter Plot: Price vs Quantity vs ID\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3810d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Parallel Coordinates Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3084f0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "# Converting the PySpark DataFrame to Pandas DataFrame\n",
    "pandas_df = df.select(\"Product\", \"Price\", \"Quantity\").toPandas()\n",
    "\n",
    "# Plotting a parallel coordinates plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "parallel_coordinates(pandas_df, \"Product\", colormap=plt.get_cmap(\"Set1\"))\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.title(\"Parallel Coordinates Plot: Product, Price, Quantity\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bd591b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee6f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bubble Chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16e8bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extracting the X, Y, and size values\n",
    "x_values = df.select(\"ID\").rdd.flatMap(lambda x: x).collect()\n",
    "y_values = df.select(\"Price\").rdd.flatMap(lambda x: x).collect()\n",
    "size_values = df.select(\"Quantity\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Plotting a bubble chart\n",
    "plt.scatter(x_values, y_values, s=size_values, alpha=0.5)\n",
    "plt.xlabel(\"ID\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Bubble Chart: Price vs ID with Quantity as Size\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8956afb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Polar Chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7b5da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Aggregating the total sales by category\n",
    "sales_by_category = df.groupBy(\"Category\").sum(\"Price\").toPandas()\n",
    "\n",
    "# Creating data for the polar chart\n",
    "categories = sales_by_category[\"Category\"]\n",
    "sales_values = sales_by_category[\"sum(Price)\"]\n",
    "\n",
    "# Plotting a polar chart\n",
    "angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "sales_values += sales_values[:1]\n",
    "angles += angles[:1]\n",
    "plt.polar(angles, sales_values)\n",
    "plt.xticks(angles[:-1], categories)\n",
    "plt.title(\"Polar Chart: Total Sales by Category\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06ddef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Donut Chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d883867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculating the total sales for each category\n",
    "sales_by_category = df.groupBy(\"Category\").sum(\"Price\").toPandas()\n",
    "\n",
    "# Plotting a donut chart of total sales by category\n",
    "plt.pie(sales_by_category[\"sum(Price)\"], labels=sales_by_category[\"Category\"], autopct='%1.1f%%', wedgeprops=dict(width=0.4))\n",
    "plt.title(\"Donut Chart: Total Sales by Category\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4640b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "Streamgraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd13249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Converting the PySpark DataFrame to Pandas DataFrame\n",
    "pandas_df = df.groupBy(\"Category\").sum(\"Price\").toPandas()\n",
    "\n",
    "# Sorting the data by category\n",
    "pandas_df.sort_values(\"Category\", inplace=True)\n",
    "\n",
    "# Creating a streamgraph\n",
    "plt.stackplot(pandas_df[\"Category\"], pandas_df.drop(\"Category\", axis=1).T, labels=pandas_df.drop(\"Category\", axis=1).columns)\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.title(\"Streamgraph: Total Sales by Category\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ea8576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf8333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visually interactive Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621d83a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Interactive Line Chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19d4ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Converting the PySpark DataFrame to Pandas DataFrame\n",
    "pandas_df = df.select(\"ID\", \"Price\").toPandas()\n",
    "\n",
    "# Creating an interactive line chart\n",
    "fig = px.line(pandas_df, x=\"ID\", y=\"Price\", title=\"Interactive Line Chart: Price by ID\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ab4a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Interactive Scatter Plot with Tooltips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7b1130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Converting the PySpark DataFrame to Pandas DataFrame\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "# Creating an interactive scatter plot with tooltips\n",
    "fig = px.scatter(pandas_df, x=\"Price\", y=\"Quantity\", hover_data=[\"Product\"], title=\"Interactive Scatter Plot with Tooltips\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8adf0ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddd9e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "Interactive Bar Chart with Dropdown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe26c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Converting the PySpark DataFrame to Pandas DataFrame\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "# Creating an interactive bar chart with dropdown\n",
    "fig = px.bar(pandas_df, x=\"Product\", y=\"Price\", color=\"Category\", title=\"Interactive Bar Chart with Dropdown\")\n",
    "fig.update_layout(xaxis={'categoryorder':'total descending'})\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc1d0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Interactive Sunburst Chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e12b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Converting the PySpark DataFrame to Pandas DataFrame\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "# Creating an interactive sunburst chart\n",
    "fig = px.sunburst(pandas_df, path=[\"Category\", \"Product\"], values=\"Price\", title=\"Interactive Sunburst Chart: Price by Category and Product\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f158e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "Interactive Heatmap with Slider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d947045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Converting the PySpark DataFrame to Pandas DataFrame\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "# Creating an interactive heatmap with slider\n",
    "fig = px.imshow(pandas_df.pivot(index=\"Category\", columns=\"Product\", values=\"Price\"), x=pandas_df[\"Product\"], y=pandas_df[\"Category\"], title=\"Interactive Heatmap with Slider: Price by Category and Product\")\n",
    "fig.update_layout(xaxis={'categoryorder':'total descending'}, yaxis={'categoryorder':'total ascending'})\n",
    "fig.update_xaxes(side=\"top\")\n",
    "fig.update_traces(colorbar=dict(len=0.4, y=0.8))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f98618",
   "metadata": {},
   "outputs": [],
   "source": [
    "Interactive Pie Chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8192cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Converting the PySpark DataFrame to Pandas DataFrame\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "# Creating an interactive pie chart\n",
    "fig = px.pie(pandas_df, names=\"Category\", title=\"Interactive Pie Chart: Distribution by Category\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a791e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "Choropleth Map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4620638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Converting the PySpark DataFrame to Pandas DataFrame\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "# Creating a choropleth map\n",
    "fig = px.choropleth(pandas_df, locations=\"Country\", color=\"Price\", title=\"Choropleth Map: Price by Country\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77dd40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Box Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d7efb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Converting the PySpark DataFrame to Pandas DataFrame\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "# Creating a box plot\n",
    "fig = px.box(pandas_df, x=\"Category\", y=\"Price\", title=\"Box Plot: Price Distribution by Category\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b47399",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tree Diagram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c50f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Creating a tree diagram\n",
    "fig = go.Figure(go.Treemap(\n",
    "    labels=df.select(\"Product\").rdd.flatMap(lambda x: x).collect(),\n",
    "    parents=[''] * len(df.select(\"Product\").rdd.flatMap(lambda x: x).collect()),\n",
    "    values=df.select(\"Price\").rdd.flatMap(lambda x: x).collect(),\n",
    "))\n",
    "fig.update_layout(title=\"Tree Diagram: Product Hierarchy\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789b6fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    " visually appealing charts with dynamic options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d5f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import squarify\n",
    "\n",
    "# Converting the PySpark DataFrame to Pandas DataFrame\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "# Calculating the total sales by category\n",
    "sales_by_category = pandas_df.groupby(\"Category\")[\"Price\"].sum().reset_index()\n",
    "\n",
    "# Creating a treemap\n",
    "plt.figure(figsize=(8, 6))\n",
    "squarify.plot(sizes=sales_by_category[\"Price\"], label=sales_by_category[\"Category\"], alpha=0.8)\n",
    "plt.title(\"Treemap: Total Sales by Category\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cfc7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Network Graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd199bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Converting the PySpark DataFrame to Pandas DataFrame\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "# Creating a network graph\n",
    "G = nx.from_pandas_edgelist(pandas_df, source=\"Product\", target=\"Category\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "pos = nx.spring_layout(G, k=0.3)\n",
    "nx.draw_networkx(G, pos, with_labels=True, node_color=\"skyblue\", node_size=800, edge_color=\"gray\", linewidths=0.5, font_size=10)\n",
    "plt.title(\"Network Graph: Product-Category Relationships\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2432e1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e402584",
   "metadata": {},
   "outputs": [],
   "source": [
    "Animated Bar Chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258bd08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "\n",
    "# Converting the PySpark DataFrame to Pandas DataFrame\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "# Sorting the data by price in descending order\n",
    "sorted_df = pandas_df.sort_values(\"Price\", ascending=False)\n",
    "\n",
    "# Creating an animated bar chart\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "def animate(i):\n",
    "    ax.clear()\n",
    "    ax.bar(sorted_df[\"Product\"].iloc[:i], sorted_df[\"Price\"].iloc[:i], color=\"skyblue\")\n",
    "    plt.xlabel(\"Product\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.title(\"Animated Bar Chart: Top Products by Price\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(sorted_df)+1, interval=200, blit=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171711ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "3D Scatter Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79468957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Converting the PySpark DataFrame to Pandas DataFrame\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "# Creating a 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=pandas_df[\"Product\"],\n",
    "    y=pandas_df[\"Category\"],\n",
    "    z=pandas_df[\"Price\"],\n",
    "    mode=\"markers\",\n",
    "    marker=dict(\n",
    "        size=8,\n",
    "        color=pandas_df[\"Quantity\"],\n",
    "        colorscale=\"Viridis\",\n",
    "        opacity=0.8\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(scene=dict(\n",
    "    xaxis_title=\"Product\",\n",
    "    yaxis_title=\"Category\",\n",
    "    zaxis_title=\"Price\"\n",
    "))\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f495b377",
   "metadata": {},
   "outputs": [],
   "source": [
    "Interactive Word Cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05e469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Converting the PySpark DataFrame to Pandas DataFrame\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "# Generating word frequencies\n",
    "word_frequencies = pandas_df[\"Product\"].value_counts()\n",
    "\n",
    "# Creating an interactive word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, colormap=\"Blues\").generate_from_frequencies(word_frequencies)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Interactive Word Cloud: Product Frequencies\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503e4cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca405bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Animated Scatter Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52f56ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "# Converting the PySpark DataFrame to Pandas DataFrame\n",
    "pandas_df = df.select(\"Price\", \"Quantity\").toPandas()\n",
    "\n",
    "# Creating an animated scatter plot\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(len(pandas_df)):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[pandas_df[\"Price\"][i]],\n",
    "        y=[pandas_df[\"Quantity\"][i]],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=10,\n",
    "            color=pandas_df[\"Price\"][i],\n",
    "            colorscale='Viridis',\n",
    "            showscale=True\n",
    "        ),\n",
    "        showlegend=False\n",
    "    ))\n",
    "    fig.update_layout(title_text='Animated Scatter Plot: Price vs Quantity', xaxis_title=\"Price\", yaxis_title=\"Quantity\")\n",
    "\n",
    "fig.update_layout(\n",
    "    updatemenus=[dict(\n",
    "        type=\"buttons\",\n",
    "        buttons=[\n",
    "            dict(label=\"Play\",\n",
    "                 method=\"animate\",\n",
    "                 args=[None, {\"frame\": {\"duration\": 500, \"redraw\": True},\n",
    "                              \"fromcurrent\": True}]),\n",
    "            dict(label=\"Pause\",\n",
    "                 method=\"animate\",\n",
    "                 args=[[None], {\"frame\": {\"duration\": 0, \"redraw\": False},\n",
    "                                \"mode\": \"immediate\",\n",
    "                                \"fromcurrent\": True}])\n",
    "        ],\n",
    "        active=0,\n",
    "        showactive=False,\n",
    "        x=0.1,\n",
    "        y=0,\n",
    "        xanchor=\"left\",\n",
    "        yanchor=\"bottom\"\n",
    "    )]\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b5cde5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fa2d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sunburst Chart with Animation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab5fe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Converting the PySpark DataFrame to Pandas DataFrame\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "# Creating a sunburst chart with animation\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(len(pandas_df)):\n",
    "    fig.add_trace(go.Sunburst(\n",
    "        labels=pandas_df[\"Product\"][:i+1],\n",
    "        parents=pandas_df[\"Category\"][:i+1],\n",
    "        values=pandas_df[\"Price\"][:i+1],\n",
    "        branchvalues='total',\n",
    "    ))\n",
    "\n",
    "fig.update_layout(title=\"Animated Sunburst Chart: Price by Category and Product\")\n",
    "\n",
    "fig.update_layout(\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            buttons=list([\n",
    "                dict(\n",
    "                    args=[{\"visible\": [True] * len(pandas_df)}],\n",
    "                    label=\"Play\",\n",
    "                    method=\"update\"\n",
    "                ),\n",
    "                dict(\n",
    "                    args=[{\"visible\": [False] * len(pandas_df)}],\n",
    "                    label=\"Pause\",\n",
    "                    method=\"update\"\n",
    "                )\n",
    "            ]),\n",
    "            type=\"buttons\",\n",
    "            direction=\"left\",\n",
    "            x=0.1,\n",
    "            y=1.1\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23323ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2358ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Animated Network Graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e317c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Converting the PySpark DataFrame to Pandas DataFrame\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "# Creating an animated network graph\n",
    "G = nx.Graph()\n",
    "\n",
    "for index, row in pandas_df.iterrows():\n",
    "    G.add_edge(row[\"Category\"], row[\"Product\"])\n",
    "\n",
    "pos = nx.spring_layout(G, k=0.15, seed=42)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for _ in range(30):\n",
    "    random.seed(42)\n",
    "    nx.draw(G, pos, with_labels=True, node_size=800, node_color=[random.random() for _ in range(len(G.nodes))])\n",
    "    plt.title(\"Animated Network Graph: Product-Category Relationships\")\n",
    "    plt.pause(0.2)\n",
    "    plt.clf()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f22fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "Animated Radial Bar Chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0586dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# Converting the PySpark DataFrame to Pandas DataFrame\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "# Aggregating data by category\n",
    "category_df = pandas_df.groupby(\"Category\").sum().reset_index()\n",
    "\n",
    "# Sorting the data by price in descending order\n",
    "sorted_df = category_df.sort_values(\"Price\", ascending=False)\n",
    "\n",
    "# Creating an animated radial bar chart\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "bars = ax.bar(sorted_df[\"Category\"], sorted_df[\"Price\"], color=\"skyblue\")\n",
    "\n",
    "def update(frame):\n",
    "    for i, bar in enumerate(bars):\n",
    "        bar.set_height(sorted_df[\"Price\"].iloc[i] / (frame + 1))\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update, frames=sorted_df.shape[0], interval=500, blit=False)\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Animated Radial Bar Chart: Price by Category\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8539b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaa3742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c5086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Section 1: Overview of PySparkling Water\n",
    "\n",
    "In this section, we will provide an overview of PySparkling Water, highlighting its features, the integration between Apache Spark and H2O, and discussing the benefits and use cases of PySparkling Water.\n",
    "\n",
    "1. Introduction to PySparkling Water and its features:\n",
    "\n",
    "PySparkling Water is a Python library that integrates H2O's machine learning algorithms with Apache Spark, providing a unified and scalable platform for big data analytics and machine learning.\n",
    "It combines the strengths of both Spark and H2O, leveraging Spark's distributed computing capabilities and H2O's advanced machine learning algorithms and data processing capabilities.\n",
    "PySparkling Water allows users to perform data preprocessing, build machine learning models, and perform distributed model training and evaluation using the familiar Spark programming interface.\n",
    "2. Integration between Apache Spark and H2O:\n",
    "\n",
    "PySparkling Water seamlessly integrates Apache Spark and H2O by providing a bridge between the two frameworks. It allows data to be transferred between Spark DataFrames and H2O Frames, enabling efficient data processing and model training.\n",
    "The integration enables Spark to leverage H2O's powerful machine learning algorithms and data processing capabilities, such as distributed random forests, gradient boosting machines, deep learning models, and more.\n",
    "PySparkling Water provides a high-level API that allows users to leverage the features and functionality of both Spark and H2O in a unified manner, simplifying the development and deployment of machine learning models on large-scale datasets.\n",
    "3. Benefits and use cases of PySparkling Water:\n",
    "\n",
    "Scalability: PySparkling Water leverages Spark's distributed computing capabilities, enabling the processing of large-scale datasets across a cluster of machines. This scalability makes it suitable for big data analytics and machine learning tasks.\n",
    "Advanced Machine Learning Algorithms: H2O provides a wide range of advanced machine learning algorithms that can be used in PySparkling Water. These algorithms are optimized for distributed computing and can handle large datasets efficiently.\n",
    "Easy Integration with Spark Ecosystem: PySparkling Water seamlessly integrates with the Spark ecosystem, allowing users to leverage other Spark components such as Spark SQL, MLlib, and Spark Streaming. This integration simplifies the development and deployment of end-to-end data pipelines.\n",
    "Data Preprocessing and Feature Engineering: PySparkling Water provides various data preprocessing and feature engineering capabilities, allowing users to clean and transform their data before building machine learning models.\n",
    "Use Cases: PySparkling Water is well-suited for use cases such as fraud detection, customer segmentation, predictive maintenance, recommendation systems, and other machine learning tasks that require scalable and advanced algorithms.\n",
    "In summary, PySparkling Water combines the power of Apache Spark and H2O, providing a unified platform for big data analytics and machine learning. It offers scalability, advanced machine learning algorithms, seamless integration with the Spark ecosystem, and a range of use cases for building and deploying large-scale machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b512ec37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be36760",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Introduction to PySparkling Water and its features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5ba79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "from pysparkling import *\n",
    "\n",
    "# Creating a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySparklingWaterExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Initializing H2O Context\n",
    "hc = H2OContext.getOrCreate(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0760d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "Explanation:\n",
    "\n",
    "The code imports the necessary libraries and creates a SparkSession, which is the entry point for interacting with Spark.\n",
    "The pysparkling library is imported to access the PySparkling Water functionality.\n",
    "H2OContext is used to initialize the H2O context, which establishes the integration between Spark and H2O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a19e3b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0976e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Integration between Apache Spark and H2O:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15785f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Spark DataFrame to H2O Frame\n",
    "h2o_frame = hc.as_h2o_frame(spark_df, \"h2o_frame\")\n",
    "\n",
    "# Converting H2O Frame to Spark DataFrame\n",
    "spark_df = hc.as_spark_frame(h2o_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a6ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "Explanation:\n",
    "\n",
    "The as_h2o_frame() function is used to convert a Spark DataFrame (spark_df) to an H2O Frame (h2o_frame). This enables using H2O's machine learning algorithms on the data.\n",
    "The as_spark_frame() function is used to convert an H2O Frame (h2o_frame) back to a Spark DataFrame (spark_df) if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3d4439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ce28c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d2c8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Benefits and use cases of PySparkling Water:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7d1086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a Gradient Boosting Machine model using H2O\n",
    "from h2o.estimators import H2OGradientBoostingEstimator\n",
    "\n",
    "# Converting Spark DataFrame to H2O Frame\n",
    "h2o_frame = hc.as_h2o_frame(spark_df, \"h2o_frame\")\n",
    "\n",
    "# Splitting data into train and test sets\n",
    "train, test = h2o_frame.split_frame(ratios=[0.8], seed=42)\n",
    "\n",
    "# Creating and training a Gradient Boosting Machine model\n",
    "gbm = H2OGradientBoostingEstimator()\n",
    "gbm.train(x=[\"feature1\", \"feature2\"], y=\"target\", training_frame=train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a174e04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Explanation:\n",
    "\n",
    "The code demonstrates the use of PySparkling Water for training a Gradient Boosting Machine (GBM) model.\n",
    "The Spark DataFrame (spark_df) is converted to an H2O Frame (h2o_frame) using as_h2o_frame().\n",
    "The H2O Frame is then split into train and test sets using split_frame().\n",
    "A GBM model is created using H2OGradientBoostingEstimator() and trained on the training set using the train() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c343cd48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365c285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Loading and Cleaning Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2a1ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creating a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySparklingWaterExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Loading data into a Spark DataFrame\n",
    "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Dropping rows with missing values\n",
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac88ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Explanation:\n",
    "\n",
    "The code imports the necessary libraries and creates a SparkSession.\n",
    "Data is loaded into a Spark DataFrame (df) using the read.csv() function. The header=True option assumes the first row as the column names, and inferSchema=True automatically infers the data types of the columns.\n",
    "Rows with missing values are dropped using the dropna() function, ensuring the dataset is clean and complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92004705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f956aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Feature Engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f5fe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "# Encoding categorical variables\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "df = indexer.fit(df).transform(df)\n",
    "\n",
    "# Combining features into a feature vector\n",
    "assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\"], outputCol=\"features\")\n",
    "df = assembler.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d0dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Explanation:\n",
    "\n",
    "The code demonstrates two common feature engineering techniques: encoding categorical variables and combining features into a feature vector.\n",
    "Categorical variable \"category\" is encoded using StringIndexer, which assigns a numerical index to each unique category, creating a new column \"categoryIndex\".\n",
    "Features \"feature1\" and \"feature2\" are combined into a single feature vector column \"features\" using VectorAssembler. This step is often required for feeding the data into machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194fdf71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe78f8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Scaling Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb19c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(df)\n",
    "df = scalerModel.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9dcd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "Explanation:\n",
    "\n",
    "The code demonstrates feature scaling using StandardScaler, which standardizes the features by subtracting the mean and dividing by the standard deviation.\n",
    "The \"features\" column is scaled to create a new column \"scaledFeatures\" using StandardScaler.\n",
    "Scaling features is important for algorithms that are sensitive to the scale of the features, such as distance-based methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d159f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362ae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Splitting Data into Train and Test Sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff0d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into train and test sets\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdc43eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Explanation:\n",
    "\n",
    "The code splits the DataFrame (df) into train and test sets using the randomSplit() function.\n",
    "The first argument [0.8, 0.2] specifies the proportions for train and test sets, respectively.\n",
    "The optional seed argument ensures reproducibility by setting a random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eddf94a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b41293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creating a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySparkExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Loading data into a Spark DataFrame\n",
    "data = [\n",
    "    (1.0, 3.5, \"A\", 0),\n",
    "    (2.0, 4.2, \"B\", 1),\n",
    "    (3.0, 6.1, \"C\", 0),\n",
    "    (4.0, 5.7, \"A\", 1),\n",
    "    (5.0, 2.8, \"B\", 0),\n",
    "    (6.0, 4.9, \"C\", 1),\n",
    "    (7.0, 3.2, \"A\", 0),\n",
    "    (8.0, 5.5, \"B\", 1),\n",
    "    (9.0, 2.9, \"C\", 0),\n",
    "    (10.0, 4.1, \"A\", 1)\n",
    "]\n",
    "\n",
    "columns = [\"feature1\", \"feature2\", \"category\", \"target\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa526e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Loading and Preprocessing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372e8ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pysparkling import *\n",
    "\n",
    "# Creating a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySparklingWaterExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Initializing H2O Context\n",
    "hc = H2OContext.getOrCreate(spark)\n",
    "\n",
    "# Loading data into a Spark DataFrame\n",
    "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Dropping rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Converting Spark DataFrame to H2O Frame\n",
    "h2o_frame = hc.as_h2o_frame(df, \"h2o_frame\")\n",
    "\n",
    "# Splitting data into train and test sets\n",
    "train, test = h2o_frame.split_frame(ratios=[0.8], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324121cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Training a Machine Learning Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bbef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required H2O libraries\n",
    "from h2o.estimators import H2OGradientBoostingEstimator\n",
    "\n",
    "# Creating and training a Gradient Boosting Machine model\n",
    "gbm = H2OGradientBoostingEstimator()\n",
    "gbm.train(x=[\"feature1\", \"feature2\"], y=\"target\", training_frame=train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68a9ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def127be",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Evaluating the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998a665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on the test set\n",
    "predictions = gbm.predict(test)\n",
    "\n",
    "# Converting H2O Frame to Spark DataFrame\n",
    "predicted_df = hc.as_spark_frame(predictions)\n",
    "\n",
    "# Calculating evaluation metrics\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"target\")\n",
    "auc = evaluator.evaluate(predicted_df)\n",
    "\n",
    "# Printing the AUC score\n",
    "print(\"AUC: \", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb70b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Explanation:\n",
    "\n",
    "The code starts with the necessary setup, including creating a SparkSession, initializing the H2O context, loading and cleaning the data, and converting the Spark DataFrame to an H2O Frame using as_h2o_frame().\n",
    "The data is then split into train and test sets using split_frame().\n",
    "A Gradient Boosting Machine (GBM) model is created using H2OGradientBoostingEstimator() and trained on the training set using the train() function.\n",
    "To evaluate the model, predictions are made on the test set using predict(). The H2O Frame of predictions is then converted back to a Spark DataFrame using as_spark_frame().\n",
    "An evaluation metric, in this case, the Area Under the ROC Curve (AUC), is calculated using BinaryClassificationEvaluator().\n",
    "The AUC score is printed to assess the performance of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b836d38c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9583d035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80ddc57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2724e0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "from pysparkling import H2OContext\n",
    "from h2o.estimators import H2ORandomForestEstimator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Creating an H2OContext\n",
    "hc = H2OContext.getOrCreate(spark)\n",
    "\n",
    "# Converting the DataFrame to an H2O Frame\n",
    "h2o_frame = hc.as_h2o_frame(df)\n",
    "\n",
    "# Splitting the H2O Frame into train and test sets\n",
    "train, test = h2o_frame.split_frame(ratios=[0.8], seed=42)\n",
    "\n",
    "# Defining the features and target columns\n",
    "feature_cols = [\"feature1\", \"feature2\", \"category\"]\n",
    "target_col = \"target\"\n",
    "\n",
    "# Converting the target column to a factor\n",
    "train[target_col] = train[target_col].asfactor()\n",
    "test[target_col] = test[target_col].asfactor()\n",
    "\n",
    "# Creating a StringIndexer for the category column\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "\n",
    "# Creating a VectorAssembler for the feature columns\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Creating an H2O Random Forest Estimator\n",
    "rf = H2ORandomForestEstimator(labelCol=target_col, featuresCol=\"features\", ntrees=100)\n",
    "\n",
    "# Creating a Pipeline with the feature transformations and the Random Forest Estimator\n",
    "pipeline = Pipeline(stages=[indexer, assembler, rf])\n",
    "\n",
    "# Training the model\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Converting the H2O Frame of predictions to a Spark DataFrame\n",
    "predicted_df = hc.as_spark_frame(predictions)\n",
    "\n",
    "# Calculating evaluation metrics\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=target_col)\n",
    "auc = evaluator.evaluate(predicted_df)\n",
    "\n",
    "# Printing the AUC score\n",
    "print(\"AUC: \", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5279041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this example, we first create an H2OContext using H2OContext.getOrCreate(spark) to initialize the H2O environment within PySpark. Then, we convert the PySpark DataFrame (df) to an H2O Frame using hc.as_h2o_frame(df).\n",
    "\n",
    "Next, we split the H2O Frame into train and test sets. We define the feature columns (feature_cols) and the target column (target_col). The target column is converted to a factor using the asfactor() method.\n",
    "\n",
    "We then create a StringIndexer for the categorical column, a VectorAssembler for the feature columns, and an H2ORandomForestEstimator as the machine learning model.\n",
    "\n",
    "A Pipeline is created with the feature transformations and the Random Forest Estimator. The model is trained using the fit() method on the train set.\n",
    "\n",
    "Predictions are made on the test set using the trained model, and the resulting H2O Frame of predictions is converted back to a Spark DataFrame using hc.as_spark_frame(). Finally, the AUC score is calculated using the BinaryClassificationEvaluator and printed to evaluate the model's performance.\n",
    "\n",
    "Make sure to run the previous code snippets for data loading and preprocessing before running this PySparkling Water machine learning example. Also, modify the feature columns, target column, and adjust the machine learning model based on your specific dataset and requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93f069c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0def8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27093592",
   "metadata": {},
   "outputs": [],
   "source": [
    "PySparkling Water is a library that provides integration between Apache Spark and H2O, an open-source machine learning platform. It allows users to combine the scalability and distributed computing capabilities of Spark with the advanced machine learning algorithms and data processing capabilities of H2O.\n",
    "\n",
    "Pre-processing and data wrangling are essential steps in preparing data for machine learning. Here are some common pre-processing and data wrangling operations along with their corresponding code examples using PySpark:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
